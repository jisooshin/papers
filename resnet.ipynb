{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T07:29:54.957699Z",
     "start_time": "2019-11-04T07:29:54.170788Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T07:29:55.436638Z",
     "start_time": "2019-11-04T07:29:55.432006Z"
    }
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T07:29:55.864698Z",
     "start_time": "2019-11-04T07:29:55.859849Z"
    }
   },
   "outputs": [],
   "source": [
    "base_path = os.environ['HOME'] + '/cifar100/test'\n",
    "if os.path.isdir(base_path):\n",
    "  pass\n",
    "else:\n",
    "  os.makedirs(base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T07:29:59.488283Z",
     "start_time": "2019-11-04T07:29:56.237839Z"
    }
   },
   "outputs": [],
   "source": [
    "writer = SummaryWriter(log_dir=base_path + '/runs/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T07:30:53.302911Z",
     "start_time": "2019-11-04T07:29:59.509794Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to /Users/shinjisoo/cifar100/test/data/cifar-100-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /Users/shinjisoo/cifar100/test/data/cifar-100-python.tar.gz to /Users/shinjisoo/cifar100/test/data\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.Pad(padding=(2, 2, 2, 2)), \n",
    "     transforms.RandomCrop(size=32),\n",
    "     torchvision.transforms.RandomHorizontalFlip(p=0.5),\n",
    "     torchvision.transforms.Resize(size=[224, 224]),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize(\n",
    "       mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))])\n",
    "train = torchvision.datasets.CIFAR100(\n",
    "  root=base_path + '/data', train=True, download=True, transform=transform) \n",
    "test = torchvision.datasets.CIFAR100(\n",
    "  root=base_path + '/data', train=False, download=True, transform=transform) \n",
    "\n",
    "trainlist = torch.utils.data.random_split(train, [40000, 10000])\n",
    "train, val = trainlist[0], trainlist[1]\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "  train, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "valloader = torch.utils.data.DataLoader(\n",
    "  val, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "  test, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T07:30:53.365495Z",
     "start_time": "2019-11-04T07:30:53.323040Z"
    }
   },
   "outputs": [],
   "source": [
    "# ResNet\n",
    "class ResNet34(nn.Module):\n",
    "  \"\"\"\n",
    "  input tensor shape. (batch, 3, 224, 224)\n",
    "  output logit shape. (batch, 100)\n",
    "  \"\"\"\n",
    "  def __init__(self):\n",
    "    super(ResNet34, self).__init__()\n",
    "    self.conv1 = nn.Conv2d(\n",
    "      in_channels=3, out_channels=64, kernel_size=(7, 7), stride=2)\n",
    "    # batch x 64 x 109 x 109\n",
    "    self.maxpool = nn.MaxPool2d(kernel_size=(3, 3), stride=2)\n",
    "    # batch x 64 x 54 x 54\n",
    " \n",
    "    \n",
    "    self.idt2 = nn.Identity()\n",
    "    # batch x 64 x 54 x 54\n",
    "    self.conv2list = [\n",
    "      nn.Conv2d(64, 64, (3, 3), padding=1) for x in range(2)]\n",
    "    # batch x 64 x 54 x 54\n",
    "    \n",
    "    self.conv2to3 = nn.Conv2d(64, 128, (3, 3), padding=1)\n",
    "    # batch x 128 x 54 x 54 \n",
    "    \n",
    "    self.idt3 = nn.Identity()\n",
    "    # batch x 128 x 54 x 54\n",
    "    self.conv3list = [\n",
    "      nn.Conv2d(128, 128, (3, 3), padding=1) for x in range(2)]\n",
    "    # batch x 128 x 54 x 54\n",
    "    self.conv3to4 = nn.Conv2d(128, 256, (3, 3), padding=1)\n",
    "    # batch x 256 x 54 x 54\n",
    "    \n",
    "    self.idt4 = nn.Identity()\n",
    "    # batch x 256 x 54 x 54\n",
    "    self.conv4list = [\n",
    "      nn.Conv2d(256, 256, (3, 3), padding=1) for x in range(2)]\n",
    "    # batch x 256 x 54 x 54\n",
    "    self.conv4to5 = nn.Conv2d(256, 512, (3, 3))\n",
    "    # batch x 512 x 54 x 54\n",
    "    \n",
    "    self.idt5 = nn.Identity()\n",
    "    # batch x 512 x 54 x 54\n",
    "    self.conv5list = [\n",
    "      nn.Conv2d(512, 512, (3, 3), padding=1) for x in range(2)]\n",
    "    # batch x 512 x 54 x 54\n",
    "\n",
    "    self.batchnorm1 = nn.BatchNorm2d(num_features=64)\n",
    "    self.batchnorm2 = nn.BatchNorm2d(num_features=128)\n",
    "    self.batchnorm3 = nn.BatchNorm2d(num_features=256)\n",
    "    self.batchnorm4 = nn.BatchNorm2d(num_features=512)\n",
    "    \n",
    "    # global average pooling 에서 나오는 최종 output은 [batch, 512]가 되도록\n",
    "    self.globalavgpool = nn.AvgPool2d(kernel_size=(52, 52)) \n",
    "    self.fc = nn.Linear(512, 100)\n",
    "    self.softmax = nn.Softmax()\n",
    "    \n",
    "  def forward(self, x):\n",
    "    def _list_to_layer(x, layer_list, batchnorm_obj):\n",
    "      for val in layer_list:\n",
    "        x = val(x)\n",
    "        x = batchnorm_obj(x)\n",
    "        x = F.relu(x)\n",
    "      return x\n",
    "    # 1st conv, maxpool\n",
    "    x = self.conv1(x)\n",
    "    x = F.relu(self.batchnorm1(x))\n",
    "    x = self.maxpool(x)\n",
    "\n",
    "    # 2nd residual block\n",
    "    idt_2_1 = self.idt2(x)\n",
    "    x = _list_to_layer(x, self.conv2list, self.batchnorm1)\n",
    "    x = idt_2_1 + x\n",
    "    idt_2_2 = self.idt2(x)\n",
    "    x = _list_to_layer(x, self.conv2list, self.batchnorm1)\n",
    "    x = idt_2_2 + x\n",
    "    idt_2_3 = self.idt2(x)\n",
    "    x = _list_to_layer(x, self.conv2list, self.batchnorm1)\n",
    "    x = idt_2_3 + x\n",
    "    x = self.conv2to3(x)\n",
    "    \n",
    "    # 3rd redsidual block\n",
    "    idt_3_1 = self.idt3(x)\n",
    "    x = _list_to_layer(x, self.conv3list, self.batchnorm2)\n",
    "    x = idt_3_1 + x\n",
    "    idt_3_2 = self.idt3(x)\n",
    "    x = _list_to_layer(x, self.conv3list, self.batchnorm2)\n",
    "    x = idt_3_2 + x\n",
    "    idt_3_3 = self.idt3(x)\n",
    "    x = _list_to_layer(x, self.conv3list, self.batchnorm2)\n",
    "    x = idt_3_3 + x\n",
    "    idt_3_4 = self.idt3(x)\n",
    "    x = _list_to_layer(x, self.conv3list, self.batchnorm2)\n",
    "    x = idt_3_4 + x\n",
    "    x = self.conv3to4(x)\n",
    "    \n",
    "    # 4th residual block\n",
    "    idt_4_1 = self.idt4(x)\n",
    "    x = _list_to_layer(x, self.conv4list, self.batchnorm3)\n",
    "    x = idt_4_1 + x\n",
    "    idt_4_2 = self.idt4(x)\n",
    "    x = _list_to_layer(x, self.conv4list, self.batchnorm3)\n",
    "    x = idt_4_2 + x\n",
    "    idt_4_3 = self.idt4(x)\n",
    "    x = _list_to_layer(x, self.conv4list, self.batchnorm3)\n",
    "    x = idt_4_3 + x\n",
    "    idt_4_4 = self.idt4(x)\n",
    "    x = _list_to_layer(x, self.conv4list, self.batchnorm3)\n",
    "    x = idt_4_4 + x\n",
    "    idt_4_5 = self.idt4(x)\n",
    "    x = _list_to_layer(x, self.conv4list, self.batchnorm3)\n",
    "    x = idt_4_5 + x\n",
    "    idt_4_6 = self.idt4(x)\n",
    "    x = _list_to_layer(x, self.conv4list, self.batchnorm3)\n",
    "    x = idt_4_6 + x\n",
    "    x = self.conv4to5(x)\n",
    "    \n",
    "    # 5th residual block\n",
    "    idt_5_1 = self.idt5(x)\n",
    "    x = _list_to_layer(x, self.conv5list, self.batchnorm4)\n",
    "    x = idt_5_1 + x\n",
    "    idt_5_2 = self.idt5(x)\n",
    "    x = _list_to_layer(x, self.conv5list, self.batchnorm4)\n",
    "    x = idt_5_2 + x\n",
    "    idt_5_3 = self.idt5(x)\n",
    "    x = _list_to_layer(x, self.conv5list, self.batchnorm4)\n",
    "    x = idt_5_3 + x\n",
    "    \n",
    "    x = self.globalavgpool(x).view([-1, 512])\n",
    "    x = self.fc(x)\n",
    "    x = self.softmax(x)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T07:30:53.473821Z",
     "start_time": "2019-11-04T07:30:53.390455Z"
    }
   },
   "outputs": [],
   "source": [
    "resnet = ResNet34()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T07:59:38.615350Z",
     "start_time": "2019-11-04T07:59:38.610349Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet34(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2))\n",
       "  (maxpool): MaxPool2d(kernel_size=(3, 3), stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (idt2): Identity()\n",
       "  (conv2to3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (idt3): Identity()\n",
       "  (conv3to4): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (idt4): Identity()\n",
       "  (conv4to5): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (idt5): Identity()\n",
       "  (batchnorm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (batchnorm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (batchnorm3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (batchnorm4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (globalavgpool): AvgPool2d(kernel_size=(52, 52), stride=(52, 52), padding=0)\n",
       "  (fc): Linear(in_features=512, out_features=100, bias=True)\n",
       "  (softmax): Softmax(dim=None)\n",
       ")"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-02T10:53:47.364781Z",
     "start_time": "2019-11-02T10:53:42.902Z"
    }
   },
   "outputs": [],
   "source": [
    "hypothesis = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(resnet.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-02T10:53:47.366380Z",
     "start_time": "2019-11-02T10:53:42.902Z"
    }
   },
   "outputs": [],
   "source": [
    "for epoch in range(10):\n",
    "  running_loss = 0.\n",
    "  for i, data in enumerate(trainloader):\n",
    "    inputs, labels = data\n",
    "    optimizer.zero_grad()\n",
    "    logits = resnet(inputs)\n",
    "    loss = hypothesis(logits, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    running_loss += loss.item()\n",
    "    \n",
    "    if i % 10 == 9:\n",
    "      val_inputs, val_labels = next(iter(valloader))\n",
    "      val_logits = resnet(val_inputs)\n",
    "      val_loss = hypothesis(val_logits, val_labels)\n",
    "      correct_case = (val_logits.max(dim=1)[1] == val_labels).sum()\n",
    "      all_case = val_labels.shape[0]\n",
    "      accuracy = (correct_case/all_case)*100\n",
    "      print(\n",
    "        \"Epoch: {} / step: {}\".format(epoch, i), \n",
    "        \"train loss:  {0:.3f}\".format(loss), \"\\t\",\n",
    "        \"Epoch: {} / step: {}\".format(epoch, i),\n",
    "        \"validation accuracy: {0:.3f}%\".format(accuracy))\n",
    "      writer.add_scalar(\n",
    "        'training loss', loss.item(), epoch*len(trainloader)+i)\n",
    "      writer.add_scalar(\n",
    "        'validation loss', val_loss.item(), epoch*len(trainloader)+i)\n",
    "      writer.add_scalar(\n",
    "        'accuracy', accuracy, epoch*len(trainloader)+i)\n",
    "      \n",
    "      torch.save({'epoch': epoch, 'model_state_dict': resnet.state_dict(),\n",
    "          'optimizer_state_dict': optimizer.state_dict(),\n",
    "          'loss': loss}, base_path + \"/resnet34_{}.pt\".format(epoch))\n",
    "      running_loss = 0.\n",
    "print(\"Finish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
