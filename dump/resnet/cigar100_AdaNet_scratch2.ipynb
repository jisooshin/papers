{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-25T06:50:25.646028Z",
     "start_time": "2019-10-25T06:50:23.982291Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-25T06:50:25.652029Z",
     "start_time": "2019-10-25T06:50:25.648520Z"
    }
   },
   "outputs": [],
   "source": [
    "base_path = os.environ['HOME'] + '/cifar100/AdaNet'\n",
    "if os.path.isdir(base_path):\n",
    "  pass\n",
    "else:\n",
    "  os.mkdir(base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-25T06:50:26.752007Z",
     "start_time": "2019-10-25T06:50:25.654189Z"
    }
   },
   "outputs": [],
   "source": [
    "data = tf.keras.datasets.cifar100.load_data()\n",
    "\n",
    "train = data[0]\n",
    "test = data[1]\n",
    "\n",
    "train_image, train_label = train[0].astype(np.float32), train[1]\n",
    "test_image, test_label= test[0].astype(np.float32), test[1]\n",
    "\n",
    "train_label = np.reshape(train_label, newshape=[-1])\n",
    "test_label = np.reshape(test_label, newshape=[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-25T06:50:27.746550Z",
     "start_time": "2019-10-25T06:50:26.754182Z"
    }
   },
   "outputs": [],
   "source": [
    "# Generate Dataset obj   \n",
    "dataset_obj = tf.data.Dataset.from_tensors(\n",
    "    {'image': train_image, 'label': train_label})\n",
    "#dataset_obj = dataset_obj.shuffle(50000)\n",
    "dataset_obj = dataset_obj.unbatch()\n",
    "\n",
    "# split train-validation dataset\n",
    "train_dataset = dataset_obj.take(40000)\n",
    "val_dataset = dataset_obj.skip(40000).take(10000)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensors(\n",
    "  {'image': test_image, 'label': test_label})\n",
    "test_dataset = test_dataset.shuffle(10000).unbatch()\n",
    "\n",
    "def _preprocessing(dataset, train_mode):\n",
    "  \"\"\"\n",
    "  While train steps, image will be padded random crop and filped(horizontaly)\n",
    "  And entire steps, per-pixel mean subtracted will be required.\n",
    "  Args:\n",
    "    dataset: 'tf.data.Dataset'\n",
    "    train_mode: 'bool'\n",
    "  Returns:\n",
    "    'tf.data.Dataset'\n",
    "  \"\"\"\n",
    "  if train_mode:\n",
    "    image = dataset['image']\n",
    "    pad = tf.constant([[2, 2], [2, 2], [0, 0]])\n",
    "    image = tf.pad(\n",
    "      tensor=image, paddings=pad)\n",
    "    image = tf.image.random_crop(\n",
    "      value=image, size=[32, 32, 3])\n",
    "    image = tf.image.random_flip_left_right(image=image)\n",
    "  else:\n",
    "    image = dataset['image']\n",
    "    \n",
    "  image = tf.math.subtract(\n",
    "    x=image,\n",
    "    y=tf.reshape(\n",
    "      tf.math.reduce_mean(image, axis=2), shape=[32, 32, 1]))\n",
    "  label = dataset['label']\n",
    "  return (image, label)\n",
    "\n",
    "train_dataset = train_dataset.map(\n",
    "  lambda x: _preprocessing(x, train_mode=True))\n",
    "val_dataset = val_dataset.map(\n",
    "  lambda x: _preprocessing(x, train_mode=False))\n",
    "test_dataset = test_dataset.map(\n",
    "  lambda x: _preprocessing(x, train_mode=False))\n",
    "\n",
    "\n",
    "train_dataset = train_dataset.shuffle(10000)\n",
    "val_dataset = val_dataset.shuffle(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-25T06:50:27.765314Z",
     "start_time": "2019-10-25T06:50:27.749052Z"
    }
   },
   "outputs": [],
   "source": [
    "class ResidualBlock(keras.layers.Layer):\n",
    "  def __init__(self, constraint, **kwargs):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      constraint(int): Max constraint for variable. (upper lambda)\n",
    "    \"\"\"\n",
    "    super(ResidualBlock, self).__init__()\n",
    "    self.conv_idt = keras.layers.Conv2D(\n",
    "      filters=3, kernel_size=[1, 1],\n",
    "      strides=1, padding='same', name='ConvIdt',\n",
    "      kernel_constraint=keras.constraints.MaxNorm(constraint))\n",
    "    self.bn_idt = keras.layers.BatchNormalization()\n",
    "    self.relu_idt = keras.layers.ReLU()\n",
    "    \n",
    "    self.conv_1 = keras.layers.Conv2D(\n",
    "      filters=64, kernel_size=[1, 1],\n",
    "      strides=1, padding='same', name='Conv1',\n",
    "      kernel_constraint=keras.constraints.MaxNorm(constraint))\n",
    "    self.bn_1 = keras.layers.BatchNormalization()\n",
    "    self.relu_1 = keras.layers.ReLU()\n",
    "    \n",
    "    self.conv_2 = keras.layers.Conv2D(\n",
    "      filters=64, kernel_size=[1, 1],\n",
    "      strides=1, padding='same', name='Conv2',\n",
    "      kernel_constraint=keras.constraints.MaxNorm(constraint))\n",
    "    self.bn_2 = keras.layers.BatchNormalization()\n",
    "    self.relu_2 = keras.layers.ReLU()\n",
    "    \n",
    "    self.conv_3 = keras.layers.Conv2D(\n",
    "      filters=3, kernel_size=[1, 1],\n",
    "      strides=1, padding='same', name='Conv3',\n",
    "      kernel_constraint=keras.constraints.MaxNorm(constraint))\n",
    "    self.bn_3 = keras.layers.BatchNormalization()\n",
    "    self.relu_3 = keras.layers.ReLU()\n",
    "    \n",
    "    self.add_last = keras.layers.Add()\n",
    "    self.relu_last = keras.layers.ReLU()\n",
    "    \n",
    "  def call(self, inputs):\n",
    "    idt_x = self.conv_idt(inputs)\n",
    "    idt_x = self.bn_idt(idt_x)\n",
    "    idt_x = self.relu_idt(idt_x)\n",
    "    x = self.conv_1(inputs)\n",
    "    x = self.bn_1(x)\n",
    "    x = self.relu_1(x)\n",
    "    x = self.conv_2(x)\n",
    "    x = self.bn_2(x)\n",
    "    x = self.relu_2(x)\n",
    "    x = self.conv_3(x)\n",
    "    x = self.bn_3(x)\n",
    "    x = self.relu_3(x)\n",
    "    x = self.add_last([x, idt_x])\n",
    "    return self.relu_last(x)\n",
    "  \n",
    "  \n",
    "class AdaNetLoss(keras.losses.Loss):\n",
    "  def __init__(self,\n",
    "               #weight,\n",
    "               #num_outputs, \n",
    "               #batch_size, \n",
    "               #num_classes,\n",
    "               name='weighted_loss'):\n",
    "    super(AdaNetLoss, self).__init__()\n",
    "    self.weight = weight\n",
    "    #self.num_outputs = num_outputs\n",
    "    #self.batch_size = batch_size\n",
    "    #self.num_classes = num_classes\n",
    "  \n",
    "  @tf.function\n",
    "  def call(self, y_true, y_pred):\n",
    "    object_function = tf.math.subtract(\n",
    "      tf.constant(1, dtype=tf.float32),\n",
    "      tf.math.multiply(\n",
    "        tf.cast(y_pred, dtype=tf.float32), tf.cast(y_true, dtype=tf.float32)))\n",
    "\n",
    "    #regularization_term = Rademacher Complexity+\n",
    "    object_function = tf.math.log(\n",
    "      x=tf.math.add(\n",
    "        tf.constant(1, dtype=tf.float32),\n",
    "        tf.math.exp(object_function)))\n",
    "\n",
    "    #object_function = keras.metrics.Mean()(object_function)\n",
    "    return object_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-25T06:50:27.778038Z",
     "start_time": "2019-10-25T06:50:27.767934Z"
    }
   },
   "outputs": [],
   "source": [
    "# model subclassing에서 받는 __init__ args 는\n",
    "# 1. layer 갯수 \n",
    "\n",
    "#call output은 \n",
    "# 1. output_list (각 layer 별 output)\n",
    "\n",
    "\n",
    "class ResAdaNet(keras.Model):\n",
    "  \n",
    "  def __init__(self, num_layers, num_classes, name='resadanet'):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      num_layers(int): \n",
    "      weight(tf.Variable): Trainable. Weight for outputs list.\n",
    "      num_classes(int):\n",
    "    \"\"\"\n",
    "    super(ResAdaNet, self).__init__()\n",
    "    self.num_layers = num_layers\n",
    "    self.residual_block = ResidualBlock(constraint=2)\n",
    "    self.num_classes = num_classes\n",
    "    \n",
    "  def call(self, inputs):\n",
    "    if self.num_layers == 0: \n",
    "      x = self.residual_block(inputs)\n",
    "      _output = keras.layers.GlobalAveragePooling2D()(x)\n",
    "      _output = keras.layers.Dense(\n",
    "        self.num_classes, name=\"first_iter_output\")(_output)\n",
    "      output_list = [_output]\n",
    "    else:\n",
    "      output_list = [\n",
    "        \"output_\" + str(x) for x in list(range(1, self.num_layers+1))]\n",
    "      x = self.residual_block(inputs)\n",
    "      for i in range(self.num_layers):\n",
    "        x = self.residual_block(x)\n",
    "        _output = keras.layers.GlobalAveragePooling2D()(x)\n",
    "        _output = keras.layers.Dense(\n",
    "          self.num_classes, name=output_list[i])(_output)\n",
    "        output_list[i] = _output\n",
    "        \n",
    "    \"\"\"    \n",
    "    # weight * \n",
    "    weighted_pred = tf.tensordot(\n",
    "      tf.reshape(self.weight, shape=[self.num_layers, 1]),\n",
    "      tf.reshape(\n",
    "        output_list, shape=[self.num_layers, -1, self.num_classes]),\n",
    "      axes=[[0], [0]])\n",
    "    weighted_pred_mean = tf.math.multiply(\n",
    "      weighted_pred, 1/self.num_layers)\n",
    "    \"\"\"\n",
    "    outputs = tf.reshape(\n",
    "      output_list, shape=[self.num_layers, -1, self.num_classes])\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-25T06:50:27.784661Z",
     "start_time": "2019-10-25T06:50:27.782401Z"
    }
   },
   "outputs": [],
   "source": [
    "# PARAMS\n",
    "NUM_LAYERS = 10\n",
    "NUM_CLASSES = 100\n",
    "EPOCHS = 3\n",
    "BATCH_SIZE = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-25T06:50:27.792929Z",
     "start_time": "2019-10-25T06:50:27.787661Z"
    }
   },
   "outputs": [],
   "source": [
    "weight = tf.Variable(\n",
    "  tf.ones(shape=[NUM_LAYERS, 1], dtype=tf.float32),\n",
    "  trainable=True, name='Weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-25T06:50:27.797217Z",
     "start_time": "2019-10-25T06:50:27.794518Z"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-25T06:50:27.812140Z",
     "start_time": "2019-10-25T06:50:27.799030Z"
    }
   },
   "outputs": [],
   "source": [
    "model = ResAdaNet(\n",
    "  num_layers=NUM_LAYERS,\n",
    "  num_classes=NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-25T06:55:34.710294Z",
     "start_time": "2019-10-25T06:50:27.814188Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of epoch: 0\n",
      "Step0: Loss 12.73680305480957\n",
      "tf.Tensor([93 18 11 62 45 52 36 64 95 73], shape=(10,), dtype=int64)\n",
      "predicted: tf.Tensor([11  2  4 26  4 26  2  2  9 11], shape=(10,), dtype=int64)\n",
      "Step100: Loss 5.334727764129639\n",
      "tf.Tensor([79 17 76 78 40 17 48  7 38 94], shape=(10,), dtype=int64)\n",
      "predicted: tf.Tensor([17 17  0  8  0  0  0  0 17 17], shape=(10,), dtype=int64)\n",
      "Step200: Loss 4.605170249938965\n",
      "tf.Tensor([24 25 72 39 26 68 11 28  5 54], shape=(10,), dtype=int64)\n",
      "predicted: tf.Tensor([0 0 0 0 0 0 0 0 0 0], shape=(10,), dtype=int64)\n",
      "Step300: Loss 4.605170249938965\n",
      "tf.Tensor([29 58 48 61 11  3 14 25 68 73], shape=(10,), dtype=int64)\n",
      "predicted: tf.Tensor([0 0 0 0 0 0 0 0 0 0], shape=(10,), dtype=int64)\n",
      "Step400: Loss 4.605170249938965\n",
      "tf.Tensor([56 85 21 14 23 63 49 49 72 40], shape=(10,), dtype=int64)\n",
      "predicted: tf.Tensor([0 0 0 0 0 0 0 0 0 0], shape=(10,), dtype=int64)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-83f6a267f575>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     grads = tape.gradient(\n\u001b[0;32m---> 28\u001b[0;31m       target=loss, sources=trainable_list)\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0;31m#print(grads[-1])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow_core/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1012\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0msources_raw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_sources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[1;32m   1015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow_core/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     74\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m       \u001b[0msources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m       compat.as_str(unconnected_gradients.value))\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow_core/python/eager/backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow_core/python/ops/nn_grad.py\u001b[0m in \u001b[0;36m_FusedBatchNormV3Grad\u001b[0;34m(op, *grad)\u001b[0m\n\u001b[1;32m    924\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRegisterGradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"FusedBatchNormV3\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    925\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_FusedBatchNormV3Grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 926\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_BaseFusedBatchNormGrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    927\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow_core/python/ops/nn_grad.py\u001b[0m in \u001b[0;36m_BaseFusedBatchNormGrad\u001b[0;34m(op, version, *grad)\u001b[0m\n\u001b[1;32m    907\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    908\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"reserve_space_3\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 909\u001b[0;31m     \u001b[0mdx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdscale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoffset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    910\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdata_format\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34mb\"NCHW\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m       \u001b[0mdx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36mfused_batch_norm_grad_v3\u001b[0;34m(y_backprop, x, scale, reserve_space_1, reserve_space_2, reserve_space_3, epsilon, data_format, is_training, name)\u001b[0m\n\u001b[1;32m   4325\u001b[0m         \u001b[0my_backprop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreserve_space_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreserve_space_2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4326\u001b[0m         \u001b[0mreserve_space_3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"epsilon\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"data_format\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4327\u001b[0;31m         \"is_training\", is_training)\n\u001b[0m\u001b[1;32m   4328\u001b[0m       \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_FusedBatchNormGradV3Output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4329\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "  print(\"Start of epoch: {}\".format(epoch))\n",
    "  \n",
    "  for step, train in enumerate(train_dataset.batch(BATCH_SIZE)):\n",
    "    with tf.GradientTape() as tape:\n",
    "      \n",
    "      trainable_list = model.trainable_weights\n",
    "      trainable_list.append(weight)\n",
    "      \n",
    "      tape.watch(trainable_list)\n",
    "      \n",
    "      train_data = train[0]\n",
    "      #train_label = tf.one_hot(train[1], depth=100)\n",
    "      train_label = train[1]\n",
    "      \n",
    "      pred = model(train_data) # (num_layer, batch, num_classes)\n",
    "      weighted_pred = tf.tensordot(\n",
    "        weight, pred, axes=[[0], [0]])\n",
    "      weighted_pred = tf.reshape(\n",
    "        weighted_pred, shape=[BATCH_SIZE, NUM_CLASSES])\n",
    "      #loss = AdaNetLoss()(y_pred=weighted_pred, y_true=train_label)\n",
    "      loss = keras.losses.SparseCategoricalCrossentropy()(y_pred=weighted_pred, y_true=train_label)\n",
    "      \n",
    "    #trainable_list = model.trainable_weights\n",
    "    #trainable_list.append(weight)\n",
    "    \n",
    "    grads = tape.gradient(\n",
    "      target=loss, sources=trainable_list)\n",
    "    #print(grads[-1])\n",
    "    optimizer.apply_gradients(zip(grads, trainable_list))\n",
    "    \n",
    "    if step % 100 == 0:\n",
    "      print(\"Step{}: Loss {}\".format(step, loss))\n",
    "      print(train[1][:10])\n",
    "      print(\"predicted:\", tf.argmax(weighted_pred)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
