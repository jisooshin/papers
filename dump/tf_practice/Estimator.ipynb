{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T04:46:59.496951Z",
     "start_time": "2019-10-07T04:46:59.492900Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import adanet\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T04:47:02.055087Z",
     "start_time": "2019-10-07T04:47:00.005620Z"
    }
   },
   "outputs": [],
   "source": [
    "data = tf.keras.datasets.cifar100.load_data()\n",
    "\n",
    "train = data[0]\n",
    "test = data[1]\n",
    "\n",
    "train_image, train_label = train[0].astype(np.float32), train[1]\n",
    "test_image, test_label= test[0].astype(np.float32), test[1]\n",
    "\n",
    "train_label = np.reshape(train_label, newshape=[-1])\n",
    "test_label = np.reshape(test_label, newshape=[-1])\n",
    "\n",
    "# Generate Dataset obj   \n",
    "dataset_obj = tf.data.Dataset.from_tensors(\n",
    "    {'image': train_image, 'label': train_label})\n",
    "dataset_obj = dataset_obj.shuffle(50000)\n",
    "dataset_obj = dataset_obj.unbatch()\n",
    "\n",
    "# split train-validation dataset\n",
    "train_dataset = dataset_obj.take(40000)\n",
    "val_dataset = dataset_obj.skip(40000).take(10000)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensors(\n",
    "  {'image': test_image, 'label': test_label})\n",
    "test_dataset = test_dataset.shuffle(10000).unbatch()\n",
    "\n",
    "def _preprocessing(dataset, train_mode):\n",
    "  \"\"\"\n",
    "  While train steps, image will be padded random crop and filped(horizontaly)\n",
    "  And entire steps, per-pixel mean subtracted will be required.\n",
    "  Args:\n",
    "    dataset: 'tf.data.Dataset'\n",
    "    train_mode: 'bool'\n",
    "  Returns:\n",
    "    'tf.data.Dataset'\n",
    "  \"\"\"\n",
    "  if train_mode:\n",
    "    image = dataset['image']\n",
    "    pad = tf.constant([[2, 2], [2, 2], [0, 0]])\n",
    "    image = tf.pad(\n",
    "      tensor=image, paddings=pad)\n",
    "    image = tf.image.random_crop(\n",
    "      value=image, size=[32, 32, 3])\n",
    "    image = tf.image.random_flip_left_right(image=image)\n",
    "  else:\n",
    "    image = dataset['image']\n",
    "    \n",
    "  label = dataset['label']\n",
    "  return (image, label)\n",
    "\n",
    "train_dataset = train_dataset.map(\n",
    "  lambda x: _preprocessing(x, train_mode=True))\n",
    "val_dataset = val_dataset.map(\n",
    "  lambda x: _preprocessing(x, train_mode=False))\n",
    "test_dataset = test_dataset.map(\n",
    "  lambda x: _preprocessing(x, train_mode=False))\n",
    "\n",
    "\n",
    "train_dataset = train_dataset.repeat()\n",
    "val_dataset = val_dataset.repeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T04:47:02.257061Z",
     "start_time": "2019-10-07T04:47:02.253354Z"
    }
   },
   "outputs": [],
   "source": [
    "base_path = os.environ['HOME'] + '/Estimator'\n",
    "if os.path.isdir(base_path):\n",
    "  pass\n",
    "else:\n",
    "  os.mkdir(base_path)\n",
    "MODEL_DIR = base_path+'/simple_nn_estimator'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T04:47:02.448113Z",
     "start_time": "2019-10-07T04:47:02.443709Z"
    }
   },
   "outputs": [],
   "source": [
    "def gen_input_fn(ds_obj, batch_size):\n",
    "  def _make():\n",
    "    return ds_obj.shuffle(1000).batch(batch_size)\n",
    "  return _make\n",
    "\n",
    "train_input = gen_input_fn(ds_obj=train_dataset, batch_size=100)\n",
    "val_input = gen_input_fn(ds_obj=val_dataset, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T04:47:02.654539Z",
     "start_time": "2019-10-07T04:47:02.650625Z"
    }
   },
   "outputs": [],
   "source": [
    "head = tf.estimator.MultiClassHead(\n",
    "  n_classes=100, loss_reduction=tf.losses.Reduction.SUM_OVER_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T04:47:02.846608Z",
     "start_time": "2019-10-07T04:47:02.843746Z"
    }
   },
   "outputs": [],
   "source": [
    "run_config = tf.estimator.RunConfig(\n",
    "  model_dir=MODEL_DIR,\n",
    "  save_summary_steps=100,\n",
    "  save_checkpoints_steps=100,\n",
    "  keep_checkpoint_max=5,\n",
    "  session_creation_timeout_secs=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T04:47:03.043590Z",
     "start_time": "2019-10-07T04:47:03.039125Z"
    }
   },
   "outputs": [],
   "source": [
    "# Training step에서 필요한 model임\n",
    "@tf.function\n",
    "def _image_normalizer(image):\n",
    "  image = tf.reshape(image, shape=[-1, 32, 32, 3])\n",
    "  image = tf.math.subtract(\n",
    "    x=image,\n",
    "    y=tf.reshape(\n",
    "        tf.math.reduce_mean(image, axis=3),\n",
    "        shape=[-1, 32, 32, 1]))\n",
    "  return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T04:47:03.388298Z",
     "start_time": "2019-10-07T04:47:03.231784Z"
    }
   },
   "outputs": [],
   "source": [
    "# test model\n",
    "inputs = keras.Input(shape=[32, 32, 3], name='input')\n",
    "\n",
    "identity_x = keras.layers.Conv2D(\n",
    "  filters=64, kernel_size=[1, 1], padding='same')(inputs)\n",
    "\n",
    "x = keras.layers.Conv2D(\n",
    "  filters=64, kernel_size=[1, 1], padding='same')(inputs)\n",
    "x = keras.layers.BatchNormalization()(x)\n",
    "x = keras.layers.ReLU()(x)\n",
    "\n",
    "x = keras.layers.Conv2D(\n",
    "  filters=64, kernel_size=[1, 1], padding='same')(x)\n",
    "x = keras.layers.BatchNormalization()(x)\n",
    "x = keras.layers.ReLU()(x)\n",
    "\n",
    "x = keras.layers.Conv2D(\n",
    "  filters=64, kernel_size=[1, 1], padding='same')(x)\n",
    "x = keras.layers.BatchNormalization()(x)\n",
    "x = keras.layers.ReLU()(x)\n",
    "\n",
    "x = keras.layers.Add()([x, identity_x])\n",
    "logit = keras.layers.GlobalAveragePooling2D()(x)\n",
    "logit = keras.layers.Dense(100)(logit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T04:47:03.583725Z",
     "start_time": "2019-10-07T04:47:03.578138Z"
    }
   },
   "outputs": [],
   "source": [
    "model = keras.Model(inputs, logit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T04:47:13.475181Z",
     "start_time": "2019-10-07T04:47:13.467597Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              [(None, 32, 32, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 32, 32, 64)   256         input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 32, 32, 64)   256         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu (ReLU)                    (None, 32, 32, 64)   0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 32, 32, 64)   4160        re_lu[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 32, 32, 64)   256         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_1 (ReLU)                  (None, 32, 32, 64)   0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 32, 32, 64)   4160        re_lu_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 32, 32, 64)   256         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_2 (ReLU)                  (None, 32, 32, 64)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 32, 32, 64)   256         input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 32, 32, 64)   0           re_lu_2[0][0]                    \n",
      "                                                                 conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d (Globa (None, 64)           0           add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 100)          6500        global_average_pooling2d[0][0]   \n",
      "==================================================================================================\n",
      "Total params: 16,100\n",
      "Trainable params: 15,716\n",
      "Non-trainable params: 384\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T04:29:22.856263Z",
     "start_time": "2019-10-07T04:29:22.849977Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['summary']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x for x in dir(model) if 'summary' in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleResidualBlockBuilder(adanet.subnetwork.Builder):\n",
    "  def __init__(self, learning_rate, max_steps, seed):\n",
    "    self.LEARNING_RATE = learning_rate\n",
    "    self.MAX_STEPS = max_steps\n",
    "    self.SEED = seed\n",
    "    return\n",
    "  \n",
    "  def build_subnetwork(self, features, logits_dimension, training,\n",
    "                      iteration_step, summary, previous_ensemble=None):\n",
    "    images = list(features.values())[0]\n",
    "    \n",
    "    # generate residual block\n",
    "    x = \n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
