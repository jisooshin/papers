{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-28T12:46:53.318987Z",
     "start_time": "2019-11-28T12:46:52.854694Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import torch.distributions as dist\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-28T12:46:53.327271Z",
     "start_time": "2019-11-28T12:46:53.322447Z"
    }
   },
   "outputs": [],
   "source": [
    "base_path = os.environ[\"HOME\"] + \"/cifar100/adanet\"\n",
    "if os.path.isdir(base_path):\n",
    "  pass\n",
    "else:\n",
    "  os.makedirs(base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-28T12:46:53.338237Z",
     "start_time": "2019-11-28T12:46:53.331879Z"
    }
   },
   "outputs": [],
   "source": [
    "MAX_ITER = 2\n",
    "MAX_EPOCH = 30\n",
    "NUM_CLASSES = 100\n",
    "TRAIN_BATCH_SIZE = 100\n",
    "VAL_BATCH_SIZE = 10000\n",
    "TEST_BATCH_SIZE = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-28T12:46:56.531360Z",
     "start_time": "2019-11-28T12:46:53.340714Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "base_path = os.environ['HOME'] + '/cifar100/adanet'\n",
    "if os.path.isdir(base_path):\n",
    "  pass\n",
    "else:\n",
    "  os.makedirs(base_path)\n",
    "\n",
    "writer = SummaryWriter(log_dir=base_path + '/runs/notebook_test')\n",
    "transform = transforms.Compose(\n",
    "    [transforms.Pad(padding=(2, 2, 2, 2)), \n",
    "     transforms.RandomCrop(size=32),\n",
    "     torchvision.transforms.RandomHorizontalFlip(p=0.5),\n",
    "     torchvision.transforms.Resize(size=[224, 224]),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize(\n",
    "       mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))])\n",
    "\n",
    "train = torchvision.datasets.CIFAR100(\n",
    "  root=base_path + '/data', train=True, download=True, transform=transform) \n",
    "test = torchvision.datasets.CIFAR100(\n",
    "  root=base_path + '/data', train=False, download=True, transform=transform) \n",
    "\n",
    "trainlist = torch.utils.data.random_split(train, [40000, 10000])\n",
    "train, val = trainlist[0], trainlist[1]\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "  train, batch_size=TRAIN_BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "valloader = torch.utils.data.DataLoader(\n",
    "  val, batch_size=VAL_BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "  test, batch_size=TEST_BATCH_SIZE, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-28T12:46:56.535990Z",
     "start_time": "2019-11-28T12:46:56.533418Z"
    }
   },
   "outputs": [],
   "source": [
    "# hyperparameter\n",
    "\n",
    "# 최소단위의 block내부에서 layer들을 연결하는 weight값의 constraint\n",
    "P = 2\n",
    "INNER_LAMBDA = 0.1 # Non-negative "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-28T12:46:56.540979Z",
     "start_time": "2019-11-28T12:46:56.538721Z"
    }
   },
   "outputs": [],
   "source": [
    "# 일단 sum weight constraint는 나중에 적용하는 것으로\n",
    "# approximate하게 x 1/num_layer로 하자"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ADANET(S=((x_i, y_i)_{i=1}^{m})$)<br>\n",
    "$f_0 \\leftarrow 0$<br>\n",
    "$for\\ t \\leftarrow 1\\ to\\ T\\ do$<br>\n",
    "$\\;\\;\\;\\;\\;\\; h, h^{'} \\leftarrow WeakLearner(S, f_{t-1})$<br>\n",
    "$\\;\\;\\;\\;\\;\\; w \\leftarrow minimize(F_t(w, h))$<br>\n",
    "$\\;\\;\\;\\;\\;\\; w^{'} \\leftarrow minimize(F_t(w, h^{'}))$<br>\n",
    "$\\;\\;\\;\\;\\;\\; if \\; F_t(w, h) \\le F_t(w, h^{'}) \\; then $<br>\n",
    "$\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\; h_t \\leftarrow h$<br>\n",
    "$\\;\\;\\;\\;\\;\\; else \\;\\; h_t \\leftarrow h^{'}$<br>\n",
    "$\\;\\;\\;\\;\\;\\; if \\; F(w_{t-1}+w^{*}) < F(w_{t-1}) \\;\\; then $<br>\n",
    "$\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\; f_t\\leftarrow f_{t-1}+w^{*}\\cdot h_t$<br>\n",
    "$\\;\\;\\;\\;\\;\\; else \\;\\; return \\;\\; f_{t-1}$<br>\n",
    "$return \\;\\; f_T$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 1st iteration\n",
    "    - weaklearner를 바탕으로 $h_0, h_1$을 만든다. $h_0$은 weaklearner의 ㅛㅐ\n",
    "    - $output = w \\cdot \\mathcal{H_1}$\n",
    "    - $\\sum_{k=1}^{1}||w_k||_1 = 1$\n",
    "    - $\\mathcal{H_1} = u_1 \\cdot \\Psi(x)$ \n",
    "        - Psi(x)는 feature vector를 의미 \n",
    "        - u_1는 AdaNet Layer의 첫번째 inner weight를 의미\n",
    "2. 2nd iter's output vector\n",
    "    - $output = \\sum_{k=1}^{2}w_k \\cdot \\mathcal{H}_k$  1st iter와 동일한 shape의 output\n",
    "    - 다만 차이가 있다면 ensemble 형태로 weighted H_1, H_2의 합계가 최종 output이 됨\n",
    "    - <U>**1st iter에서 학습된 weight들 (ex. $H_1$의 weight와 bias들)은 추가 학습 없이 사용됨**</U>\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-28T12:46:56.553858Z",
     "start_time": "2019-11-28T12:46:56.544051Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# First iter 이후에는 사실상.. 껍데기 역할을 주로 하게 될 것\n",
    "# 그 이후에는 사실상의 fine tuning개념임 \n",
    "class AdaNet(nn.Module):\n",
    "  \n",
    "  def __init__(self, \n",
    "               num_layers, \n",
    "               module_instance, \n",
    "               output_instance):\n",
    "    super(AdaNet, self).__init__()\n",
    "    self.NUM_LAYERS = num_layers\n",
    "    module = [module_instance for i in range(1, num_layers)]\n",
    "    output = [output_instance for i in range(num_layers)]\n",
    "    weight = torch.ones(num_layers) / num_layers\n",
    "    self.weight = nn.Parameter(data=weight, requires_grad=True)\n",
    "    \n",
    "    self.modules_list = nn.ModuleList(module)\n",
    "    self.outputs_list = nn.ModuleList(output) \n",
    "    \"\"\"\n",
    "    output instance들은 일반적으로는 Linear를 쓸것이고 해당 Linear의 weight값들이 \n",
    "    논문에서의 u값을 의미 (l_p constraint가 적용된다.)\n",
    "    \"\"\"\n",
    "    \n",
    "  def forward(self, x):\n",
    "    output = []\n",
    "    for i in range(self.NUM_LAYERS):\n",
    "      if i == 0:\n",
    "        _output = self.outputs_list[0](x)        \n",
    "        output.append(_output)\n",
    "      else:\n",
    "        x = self.modules_list[i - 1](x) \n",
    "        _output = self.outputs_list[i](x) \n",
    "        output.append(_output)\n",
    "    output = torch.stack(output, dim=1)\n",
    "    output = torch.matmul(self.weight, output)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- base classifier는 기본적으로 Residual Block을 이용\n",
    "    -  Input - Output size가 동일하게 \n",
    "- output에서는 기본적으로 global average pooling 적용 및 Dense layer 적용 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-28T12:46:56.569406Z",
     "start_time": "2019-11-28T12:46:56.556920Z"
    }
   },
   "outputs": [],
   "source": [
    "class BaseClassifier(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(BaseClassifier, self).__init__()\n",
    "    self.conv_origin = nn.Conv2d(3, 64, (3, 3), padding=1)\n",
    "    self.conv1 = nn.Conv2d(64, 64, (3, 3), padding=1)\n",
    "    self.conv2 = nn.Conv2d(64, 64, (3, 3), padding=1)\n",
    "    self.batchnorm1 = nn.BatchNorm2d(num_features=64)\n",
    "    self.batchnorm2 = nn.BatchNorm2d(num_features=64)\n",
    "    \n",
    "  def forward(self, x):\n",
    "    if x.shape[1] == 3:\n",
    "      origin_x = self.conv_origin(x)\n",
    "      x = self.conv1(origin_x)\n",
    "      x = self.batchnorm1(x)\n",
    "      x = F.relu(x)\n",
    "      x = self.conv2(x)\n",
    "      x = self.batchnorm2(x)\n",
    "      x = F.relu(x)\n",
    "      x = torch.add(origin_x, x)\n",
    "    elif x.shape[1] == 64:\n",
    "      origin_x = x\n",
    "      x = self.conv1(x)\n",
    "      x = self.batchnorm1(x)\n",
    "      x = F.relu(x)\n",
    "      x = self.conv2(x)\n",
    "      x = self.batchnorm2(x)\n",
    "      x = F.relu(x)\n",
    "      x = torch.add(origin_x, x)\n",
    "    return x\n",
    "\n",
    "  \n",
    "class OutputModule(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(OutputModule, self).__init__()\n",
    "    self.globalavgpool = nn.AvgPool2d(kernel_size=(224, 224))\n",
    "    self.fc1 = nn.Linear(3, 100)\n",
    "    self.fc2 = nn.Linear(64, 100)\n",
    "    \n",
    "  def forward(self, x):\n",
    "    if x.shape[1] == 3:\n",
    "      x = self.globalavgpool(x).view(-1, 3)\n",
    "      logit = self.fc1(x)\n",
    "    elif x.shape[1] == 64:\n",
    "      x = self.globalavgpool(x).view(-1, 64)\n",
    "      logit = self.fc2(x)\n",
    "    return logit\n",
    "  \n",
    "# training시 running memory가 어떨지 고려하도록"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-28T12:46:56.576061Z",
     "start_time": "2019-11-28T12:46:56.571525Z"
    }
   },
   "outputs": [],
   "source": [
    "# loss function \n",
    "def criterion(trained_logits, labels, mode, training_logits=None):\n",
    "  # mode에는 multivariable model single variable mode두가지가 있음 \n",
    "  if mode == \"train\":\n",
    "    y_f = torch.mul(labels, trained_logits)\n",
    "    y_wu = torch.mul(labels, training_logits)\n",
    "    # surrogate loss\n",
    "    loss = torch.exp(1 - y_f - y_wu)\n",
    "    loss = torch.mean(loss)\n",
    "    # NOTE: Penalize term\n",
    "    # lambda = alpha*(Rademacher complexity) + beta  \n",
    "    # alpha & beta is hyperparam\n",
    "  elif mode == \"eval\":\n",
    "    y_f = torch.mul(labels, trained_logits)\n",
    "    loss = torch.exp(1 - y_f)\n",
    "    loss = torch.mean(loss)\n",
    "  else:\n",
    "    print(\"error\")\n",
    "    # NOTE: Error 적용할 것\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-28T12:46:56.579709Z",
     "start_time": "2019-11-28T12:46:56.577477Z"
    }
   },
   "outputs": [],
   "source": [
    "max_patience = 6000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-11-28T12:46:53.107Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "#------------------------------------------------------------#\n",
      "Start h\n",
      "**** ITERATION [1] ****\n",
      "Learning ** h **\n",
      "EPOCH [1] | GLOBAL STEP [100]\n",
      "Loss: 2.71693\n",
      "Min Loss: 2.71512\n",
      "Iter[1] | w[h]: Patience added: 92\n",
      "------------------------------------------------\n",
      "**** ITERATION [1] ****\n",
      "Learning ** h **\n",
      "EPOCH [1] | GLOBAL STEP [200]\n",
      "Loss: 2.71539\n",
      "Min Loss: 2.71365\n",
      "Iter[1] | w[h]: Patience added: 188\n",
      "------------------------------------------------\n",
      "**** ITERATION [1] ****\n",
      "Learning ** h **\n",
      "EPOCH [1] | GLOBAL STEP [300]\n",
      "Loss: 2.71462\n",
      "Min Loss: 2.71170\n",
      "Iter[1] | w[h]: Patience added: 284\n",
      "------------------------------------------------\n",
      "**** ITERATION [1] ****\n",
      "Learning ** h **\n",
      "EPOCH [1] | GLOBAL STEP [400]\n",
      "Loss: 2.70887\n",
      "Min Loss: 2.70887\n",
      "Iter[1] | w[h]: Patience added: 378\n",
      "------------------------------------------------\n",
      "**** ITERATION [1] ****\n",
      "Learning ** h **\n",
      "EPOCH [2] | GLOBAL STEP [500]\n",
      "Loss: 2.70900\n",
      "Min Loss: 2.70711\n",
      "Iter[1] | w[h]: Patience added: 474\n",
      "------------------------------------------------\n",
      "**** ITERATION [1] ****\n",
      "Learning ** h **\n",
      "EPOCH [2] | GLOBAL STEP [600]\n",
      "Loss: 2.70439\n",
      "Min Loss: 2.70349\n",
      "Iter[1] | w[h]: Patience added: 569\n",
      "------------------------------------------------\n",
      "**** ITERATION [1] ****\n",
      "Learning ** h **\n",
      "EPOCH [2] | GLOBAL STEP [700]\n",
      "Loss: 2.70285\n",
      "Min Loss: 2.70192\n",
      "Iter[1] | w[h]: Patience added: 667\n",
      "------------------------------------------------\n",
      "**** ITERATION [1] ****\n",
      "Learning ** h **\n",
      "EPOCH [2] | GLOBAL STEP [800]\n",
      "Loss: 2.70249\n",
      "Min Loss: 2.70065\n",
      "Iter[1] | w[h]: Patience added: 760\n",
      "------------------------------------------------\n",
      "**** ITERATION [1] ****\n",
      "Learning ** h **\n",
      "EPOCH [3] | GLOBAL STEP [900]\n",
      "Loss: 2.70030\n",
      "Min Loss: 2.69870\n",
      "Iter[1] | w[h]: Patience added: 854\n",
      "------------------------------------------------\n",
      "**** ITERATION [1] ****\n",
      "Learning ** h **\n",
      "EPOCH [3] | GLOBAL STEP [1000]\n",
      "Loss: 2.69917\n",
      "Min Loss: 2.69753\n",
      "Iter[1] | w[h]: Patience added: 949\n",
      "------------------------------------------------\n",
      "**** ITERATION [1] ****\n",
      "Learning ** h **\n",
      "EPOCH [3] | GLOBAL STEP [1100]\n",
      "Loss: 2.69810\n",
      "Min Loss: 2.69663\n",
      "Iter[1] | w[h]: Patience added: 1043\n",
      "------------------------------------------------\n",
      "**** ITERATION [1] ****\n",
      "Learning ** h **\n",
      "EPOCH [3] | GLOBAL STEP [1200]\n",
      "Loss: 2.69668\n",
      "Min Loss: 2.69567\n",
      "Iter[1] | w[h]: Patience added: 1137\n",
      "------------------------------------------------\n",
      "**** ITERATION [1] ****\n",
      "Learning ** h **\n",
      "EPOCH [4] | GLOBAL STEP [1300]\n",
      "Loss: 2.69571\n",
      "Min Loss: 2.69535\n",
      "Iter[1] | w[h]: Patience added: 1234\n",
      "------------------------------------------------\n",
      "**** ITERATION [1] ****\n",
      "Learning ** h **\n",
      "EPOCH [4] | GLOBAL STEP [1400]\n",
      "Loss: 2.69494\n",
      "Min Loss: 2.69467\n",
      "Iter[1] | w[h]: Patience added: 1327\n",
      "------------------------------------------------\n",
      "**** ITERATION [1] ****\n",
      "Learning ** h **\n",
      "EPOCH [4] | GLOBAL STEP [1500]\n",
      "Loss: 2.69484\n",
      "Min Loss: 2.69404\n",
      "Iter[1] | w[h]: Patience added: 1421\n",
      "------------------------------------------------\n",
      "**** ITERATION [1] ****\n",
      "Learning ** h **\n",
      "EPOCH [4] | GLOBAL STEP [1600]\n",
      "Loss: 2.69453\n",
      "Min Loss: 2.69354\n",
      "Iter[1] | w[h]: Patience added: 1520\n",
      "------------------------------------------------\n",
      "**** ITERATION [1] ****\n",
      "Learning ** h **\n",
      "EPOCH [5] | GLOBAL STEP [1700]\n",
      "Loss: 2.69368\n",
      "Min Loss: 2.69340\n",
      "Iter[1] | w[h]: Patience added: 1617\n",
      "------------------------------------------------\n",
      "**** ITERATION [1] ****\n",
      "Learning ** h **\n",
      "EPOCH [5] | GLOBAL STEP [1800]\n",
      "Loss: 2.69348\n",
      "Min Loss: 2.69295\n",
      "Iter[1] | w[h]: Patience added: 1713\n",
      "------------------------------------------------\n",
      "**** ITERATION [1] ****\n",
      "Learning ** h **\n",
      "EPOCH [5] | GLOBAL STEP [1900]\n",
      "Loss: 2.69340\n",
      "Min Loss: 2.69261\n",
      "Iter[1] | w[h]: Patience added: 1810\n",
      "------------------------------------------------\n",
      "**** ITERATION [1] ****\n",
      "Learning ** h **\n",
      "EPOCH [5] | GLOBAL STEP [2000]\n",
      "Loss: 2.69256\n",
      "Min Loss: 2.69256\n",
      "Iter[1] | w[h]: Patience added: 1909\n",
      "------------------------------------------------\n",
      "**** ITERATION [1] ****\n",
      "Learning ** h **\n",
      "EPOCH [6] | GLOBAL STEP [2100]\n",
      "Loss: 2.69311\n",
      "Min Loss: 2.69239\n",
      "Iter[1] | w[h]: Patience added: 2007\n",
      "------------------------------------------------\n",
      "**** ITERATION [1] ****\n",
      "Learning ** h **\n",
      "EPOCH [6] | GLOBAL STEP [2200]\n",
      "Loss: 2.69265\n",
      "Min Loss: 2.69225\n",
      "Iter[1] | w[h]: Patience added: 2105\n",
      "------------------------------------------------\n",
      "**** ITERATION [1] ****\n",
      "Learning ** h **\n",
      "EPOCH [6] | GLOBAL STEP [2300]\n",
      "Loss: 2.69237\n",
      "Min Loss: 2.69206\n",
      "Iter[1] | w[h]: Patience added: 2200\n",
      "------------------------------------------------\n",
      "**** ITERATION [1] ****\n",
      "Learning ** h **\n",
      "EPOCH [6] | GLOBAL STEP [2400]\n",
      "Loss: 2.69226\n",
      "Min Loss: 2.69205\n",
      "Iter[1] | w[h]: Patience added: 2299\n",
      "------------------------------------------------\n",
      "**** ITERATION [1] ****\n",
      "Learning ** h **\n",
      "EPOCH [7] | GLOBAL STEP [2500]\n",
      "Loss: 2.69211\n",
      "Min Loss: 2.69192\n",
      "Iter[1] | w[h]: Patience added: 2394\n",
      "------------------------------------------------\n",
      "**** ITERATION [1] ****\n",
      "Learning ** h **\n",
      "EPOCH [7] | GLOBAL STEP [2600]\n",
      "Loss: 2.69227\n",
      "Min Loss: 2.69185\n",
      "Iter[1] | w[h]: Patience added: 2489\n",
      "------------------------------------------------\n",
      "**** ITERATION [1] ****\n",
      "Learning ** h **\n",
      "EPOCH [7] | GLOBAL STEP [2700]\n",
      "Loss: 2.69188\n",
      "Min Loss: 2.69176\n",
      "Iter[1] | w[h]: Patience added: 2587\n",
      "------------------------------------------------\n",
      "**** ITERATION [1] ****\n",
      "Learning ** h **\n",
      "EPOCH [7] | GLOBAL STEP [2800]\n",
      "Loss: 2.69188\n",
      "Min Loss: 2.69170\n",
      "Iter[1] | w[h]: Patience added: 2684\n",
      "------------------------------------------------\n",
      "**** ITERATION [1] ****\n",
      "Learning ** h **\n",
      "EPOCH [8] | GLOBAL STEP [2900]\n",
      "Loss: 2.69181\n",
      "Min Loss: 2.69167\n",
      "Iter[1] | w[h]: Patience added: 2782\n",
      "------------------------------------------------\n",
      "**** ITERATION [1] ****\n",
      "Learning ** h **\n",
      "EPOCH [8] | GLOBAL STEP [3000]\n",
      "Loss: 2.69166\n",
      "Min Loss: 2.69158\n",
      "Iter[1] | w[h]: Patience added: 2878\n",
      "------------------------------------------------\n",
      "**** ITERATION [1] ****\n",
      "Learning ** h **\n",
      "EPOCH [8] | GLOBAL STEP [3100]\n",
      "Loss: 2.69177\n",
      "Min Loss: 2.69154\n",
      "Iter[1] | w[h]: Patience added: 2974\n",
      "------------------------------------------------\n",
      "**** ITERATION [1] ****\n",
      "Learning ** h **\n",
      "EPOCH [8] | GLOBAL STEP [3200]\n",
      "Loss: 2.69159\n",
      "Min Loss: 2.69151\n",
      "Iter[1] | w[h]: Patience added: 3072\n",
      "------------------------------------------------\n",
      "**** ITERATION [1] ****\n",
      "Learning ** h **\n",
      "EPOCH [9] | GLOBAL STEP [3300]\n",
      "Loss: 2.69159\n",
      "Min Loss: 2.69149\n",
      "Iter[1] | w[h]: Patience added: 3170\n",
      "------------------------------------------------\n",
      "**** ITERATION [1] ****\n",
      "Learning ** h **\n",
      "EPOCH [9] | GLOBAL STEP [3400]\n",
      "Loss: 2.69153\n",
      "Min Loss: 2.69143\n",
      "Iter[1] | w[h]: Patience added: 3266\n",
      "------------------------------------------------\n",
      "**** ITERATION [1] ****\n",
      "Learning ** h **\n",
      "EPOCH [9] | GLOBAL STEP [3500]\n",
      "Loss: 2.69148\n",
      "Min Loss: 2.69140\n",
      "Iter[1] | w[h]: Patience added: 3365\n",
      "------------------------------------------------\n",
      "**** ITERATION [1] ****\n",
      "Learning ** h **\n",
      "EPOCH [9] | GLOBAL STEP [3600]\n",
      "Loss: 2.69155\n",
      "Min Loss: 2.69139\n",
      "Iter[1] | w[h]: Patience added: 3463\n",
      "------------------------------------------------\n",
      "**** ITERATION [1] ****\n",
      "Learning ** h **\n",
      "EPOCH [10] | GLOBAL STEP [3700]\n",
      "Loss: 2.69148\n",
      "Min Loss: 2.69137\n",
      "Iter[1] | w[h]: Patience added: 3561\n",
      "------------------------------------------------\n",
      "**** ITERATION [1] ****\n",
      "Learning ** h **\n",
      "EPOCH [10] | GLOBAL STEP [3800]\n",
      "Loss: 2.69140\n",
      "Min Loss: 2.69134\n",
      "Iter[1] | w[h]: Patience added: 3659\n",
      "------------------------------------------------\n",
      "**** ITERATION [1] ****\n",
      "Learning ** h **\n",
      "EPOCH [10] | GLOBAL STEP [3900]\n",
      "Loss: 2.69137\n",
      "Min Loss: 2.69132\n",
      "Iter[1] | w[h]: Patience added: 3757\n",
      "------------------------------------------------\n",
      "**** ITERATION [1] ****\n",
      "Learning ** h **\n",
      "EPOCH [10] | GLOBAL STEP [4000]\n",
      "Loss: 2.69142\n",
      "Min Loss: 2.69130\n",
      "Iter[1] | w[h]: Patience added: 3856\n",
      "------------------------------------------------\n",
      "**** ITERATION [1] ****\n",
      "Learning ** h **\n",
      "EPOCH [11] | GLOBAL STEP [4100]\n",
      "Loss: 2.69136\n",
      "Min Loss: 2.69129\n",
      "Iter[1] | w[h]: Patience added: 3954\n",
      "------------------------------------------------\n",
      "**** ITERATION [1] ****\n",
      "Learning ** h **\n",
      "EPOCH [11] | GLOBAL STEP [4200]\n",
      "Loss: 2.69136\n",
      "Min Loss: 2.69125\n",
      "Iter[1] | w[h]: Patience added: 4053\n",
      "------------------------------------------------\n",
      "**** ITERATION [1] ****\n",
      "Learning ** h **\n",
      "EPOCH [11] | GLOBAL STEP [4300]\n",
      "Loss: 2.69130\n",
      "Min Loss: 2.69125\n",
      "Iter[1] | w[h]: Patience added: 4153\n",
      "------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** ITERATION [1] ****\n",
      "Learning ** h **\n",
      "EPOCH [11] | GLOBAL STEP [4400]\n",
      "Loss: 2.69130\n",
      "Min Loss: 2.69124\n",
      "Iter[1] | w[h]: Patience added: 4252\n",
      "------------------------------------------------\n",
      "**** ITERATION [1] ****\n",
      "Learning ** h **\n",
      "EPOCH [12] | GLOBAL STEP [4500]\n",
      "Loss: 2.69130\n",
      "Min Loss: 2.69124\n",
      "Iter[1] | w[h]: Patience added: 4351\n",
      "------------------------------------------------\n",
      "**** ITERATION [1] ****\n",
      "Learning ** h **\n",
      "EPOCH [12] | GLOBAL STEP [4600]\n",
      "Loss: 2.69124\n",
      "Min Loss: 2.69122\n",
      "Iter[1] | w[h]: Patience added: 4448\n",
      "------------------------------------------------\n",
      "**** ITERATION [1] ****\n",
      "Learning ** h **\n",
      "EPOCH [12] | GLOBAL STEP [4700]\n",
      "Loss: 2.69124\n",
      "Min Loss: 2.69121\n",
      "Iter[1] | w[h]: Patience added: 4547\n",
      "------------------------------------------------\n",
      "**** ITERATION [1] ****\n",
      "Learning ** h **\n",
      "EPOCH [12] | GLOBAL STEP [4800]\n",
      "Loss: 2.69126\n",
      "Min Loss: 2.69120\n",
      "Iter[1] | w[h]: Patience added: 4645\n",
      "------------------------------------------------\n",
      "**** ITERATION [1] ****\n",
      "Learning ** h **\n",
      "EPOCH [13] | GLOBAL STEP [4900]\n",
      "Loss: 2.69122\n",
      "Min Loss: 2.69120\n",
      "Iter[1] | w[h]: Patience added: 4743\n",
      "------------------------------------------------\n",
      "**** ITERATION [1] ****\n",
      "Learning ** h **\n",
      "EPOCH [13] | GLOBAL STEP [5000]\n",
      "Loss: 2.69120\n",
      "Min Loss: 2.69120\n",
      "Iter[1] | w[h]: Patience added: 4840\n",
      "------------------------------------------------\n",
      "**** ITERATION [1] ****\n",
      "Learning ** h **\n",
      "EPOCH [13] | GLOBAL STEP [5100]\n",
      "Loss: 2.69120\n",
      "Min Loss: 2.69118\n",
      "Iter[1] | w[h]: Patience added: 4939\n",
      "------------------------------------------------\n",
      "**** ITERATION [1] ****\n",
      "Learning ** h **\n",
      "EPOCH [13] | GLOBAL STEP [5200]\n",
      "Loss: 2.69125\n",
      "Min Loss: 2.69118\n",
      "Iter[1] | w[h]: Patience added: 5038\n",
      "------------------------------------------------\n",
      "**** ITERATION [1] ****\n",
      "Learning ** h **\n",
      "EPOCH [14] | GLOBAL STEP [5300]\n",
      "Loss: 2.69118\n",
      "Min Loss: 2.69117\n",
      "Iter[1] | w[h]: Patience added: 5137\n",
      "------------------------------------------------\n",
      "**** ITERATION [1] ****\n",
      "Learning ** h **\n",
      "EPOCH [14] | GLOBAL STEP [5400]\n",
      "Loss: 2.69118\n",
      "Min Loss: 2.69116\n",
      "Iter[1] | w[h]: Patience added: 5236\n",
      "------------------------------------------------\n",
      "**** ITERATION [1] ****\n",
      "Learning ** h **\n",
      "EPOCH [14] | GLOBAL STEP [5500]\n",
      "Loss: 2.69119\n",
      "Min Loss: 2.69116\n",
      "Iter[1] | w[h]: Patience added: 5335\n",
      "------------------------------------------------\n",
      "**** ITERATION [1] ****\n",
      "Learning ** h **\n",
      "EPOCH [14] | GLOBAL STEP [5600]\n",
      "Loss: 2.69119\n",
      "Min Loss: 2.69116\n",
      "Iter[1] | w[h]: Patience added: 5434\n",
      "------------------------------------------------\n",
      "**** ITERATION [1] ****\n",
      "Learning ** h **\n",
      "EPOCH [15] | GLOBAL STEP [5700]\n",
      "Loss: 2.69118\n",
      "Min Loss: 2.69115\n",
      "Iter[1] | w[h]: Patience added: 5531\n",
      "------------------------------------------------\n",
      "**** ITERATION [1] ****\n",
      "Learning ** h **\n",
      "EPOCH [15] | GLOBAL STEP [5800]\n",
      "Loss: 2.69115\n",
      "Min Loss: 2.69115\n",
      "Iter[1] | w[h]: Patience added: 5629\n",
      "------------------------------------------------\n",
      "**** ITERATION [1] ****\n",
      "Learning ** h **\n",
      "EPOCH [15] | GLOBAL STEP [5900]\n",
      "Loss: 2.69115\n",
      "Min Loss: 2.69114\n",
      "Iter[1] | w[h]: Patience added: 5728\n",
      "------------------------------------------------\n",
      "**** ITERATION [1] ****\n",
      "Learning ** h **\n",
      "EPOCH [15] | GLOBAL STEP [6000]\n",
      "Loss: 2.69117\n",
      "Min Loss: 2.69114\n",
      "Iter[1] | w[h]: Patience added: 5827\n",
      "------------------------------------------------\n",
      "**** ITERATION [1] ****\n",
      "Learning ** h **\n",
      "EPOCH [16] | GLOBAL STEP [6100]\n",
      "Loss: 2.69115\n",
      "Min Loss: 2.69114\n",
      "Iter[1] | w[h]: Patience added: 5926\n",
      "------------------------------------------------\n",
      "#################################################################\n",
      "Training end in global step 6192\n",
      "Minimum objective: 2.6911395\n",
      "[h] end.\n",
      "#################################################################\n",
      "#------------------------------------------------------------#\n",
      "Start h_prime\n"
     ]
    }
   ],
   "source": [
    "base_module = BaseClassifier()\n",
    "out_module = OutputModule()\n",
    "\n",
    "for t in range(1, MAX_ITER + 1): \n",
    "  # range(1, max_iter + 1) for convenience\n",
    "  print(t)\n",
    "  h = AdaNet(t, base_module, out_module)\n",
    "  h_prime = AdaNet(t + 1, base_module, out_module)  \n",
    "  weaklearners = [h, h_prime]\n",
    "  \n",
    "  # container for minimized objective value\n",
    "  min_objective = {\"h\": 0., \"h_prime\": 0.}\n",
    "  \n",
    "  for w in range(2): \n",
    "    # if w = 1 then h and otherwise h'\n",
    "    weaklearner = weaklearners[w]\n",
    "    if t == 1:\n",
    "      logit_trained = 0\n",
    "      weight_trained = 0\n",
    "    else:\n",
    "      ckpt_path = base_path + \"/{}_checkpoint.pt\".format(t - 1)\n",
    "      checkpoint = torch.load(ckpt_path)\n",
    "      print(\"Load {}\".format(ckpt_path))\n",
    "\n",
    "      prev_params = [param for param in checkpoint['model_state_dict']]\n",
    "      prev_weight_dict = {\n",
    "        k: v for k, v in checkpoint['model_state_dict'].items()\n",
    "        if k == 'weight'}\n",
    "\n",
    "      param_locate = [param for param in weaklearner.state_dict()]\n",
    "      prev_param_index = [param_locate.index(i) for i in prev_params]\n",
    "      prev_param_index.remove(0) # Weight parameter have to train \n",
    "      \n",
    "      idx = 0\n",
    "      for param in weaklearner.parameters():\n",
    "        if idx in prev_trained_param_idx:\n",
    "          param.requires_grad = False\n",
    "        else:\n",
    "          param.requires_grad = True\n",
    "        idx += 1\n",
    "    \n",
    "    optimizer = optim.Adam(params=weaklearner.parameters())\n",
    "    current_w = \"h\" if w == 0 else \"h_prime\"\n",
    "    print(\"#------------------------------------------------------------#\")\n",
    "    print(\"Start {}\".format(current_w))\n",
    "    early_metrics = []\n",
    "    min_value = 0.\n",
    "    patience = 0\n",
    "    global_steps = 0\n",
    "    \n",
    "    for epoch in range(MAX_EPOCH):\n",
    "      \n",
    "      \"\"\"\n",
    "      if t == 1:\n",
    "        logit_trained = 0\n",
    "        weight_trained = 0\n",
    "      else: \n",
    "        ckpt_path = base_path + \"/{}_checkpoint.pt\".format(t - 1)\n",
    "        checkpoint = torch.load(ckpt_path)\n",
    "        print(\"Load {}\".format(ckpt_path))\n",
    "\n",
    "        prev_weight_dict = {\n",
    "          k: v for k, v in checkpoint['model_state_dict'].items()\n",
    "          if k == 'weight'}\n",
    "        \n",
    "        # Load previous iteration function \n",
    "        # NOTE: 이전 iter에서 학습된 f_{t-1}에서 나온 logit\n",
    "        f_prev = AdaNet(t - 1, base_module, out_module)\n",
    "        f_prev.load_state_dict(checkpoint['model_state_dict'])\n",
    "        logit_trained = h_prev(inputs)\n",
    "\n",
    "        weight_trained = checkpoint['model_state_dict']['weight']\n",
    "        \"\"\"\n",
    "      if t == 1:\n",
    "        logit_trained = 0\n",
    "        weight_trained = 0\n",
    "      else:\n",
    "        f_prev = AdaNet(t - 1, base_module, out_module)\n",
    "        f_prev.load_state_dict(checkpoint['model_state_dict'], strict=False)\n",
    "        logit_trained = h_prev(inputs)\n",
    "        weight_trained = checkpoint['model_state_dict']['weight']\n",
    "\n",
    "      for i, data in enumerate(trainloader):\n",
    "        global_steps += 1\n",
    "        if patience >= max_patience:\n",
    "          break  \n",
    "        inputs, labels = data\n",
    "        _label = torch.zeros(\n",
    "          [TRAIN_BATCH_SIZE, NUM_CLASSES], dtype=torch.float32)\n",
    "        _label[range(_label.shape[0]), labels] = 1\n",
    "        labels = _label\n",
    "        optimizer.zero_grad()\n",
    "        logits = weaklearner(inputs)\n",
    "        objective = criterion(\n",
    "          trained_logits=logit_trained, \n",
    "          training_logits=logits,\n",
    "          labels=labels, mode=\"train\")\n",
    "        objective.backward()\n",
    "        optimizer.step()\n",
    " \n",
    "        del inputs, labels, data, logits, _label\n",
    "        \n",
    "        \n",
    "        ##################\n",
    "        # Early stopping #  early stop checking for every global steps.\n",
    "        ##################\n",
    "        _metric = objective.item()\n",
    "        if len(early_metrics) < 2:\n",
    "          early_metrics.append(_metric)\n",
    "        elif len(early_metrics) >= 2:\n",
    "          if _metric > min_value:\n",
    "            patience += 1\n",
    "          early_metrics.append(_metric)\n",
    "          min_value = min(early_metrics)\n",
    "          early_metrics.sort()\n",
    "          early_metrics = early_metrics[:2]\n",
    "\n",
    "        if i % 100 == 99:\n",
    "          print(\"**** ITERATION [{}] ****\".format(t))\n",
    "          if w == 0: \n",
    "            print(\"Learning ** h **\")\n",
    "          else:\n",
    "            print(\"Learning ** h_prime **\")\n",
    "          print(\n",
    "            \"EPOCH [{}] | GLOBAL STEP [{}]\".format(epoch + 1, global_steps))\n",
    "          print(\"Loss: {0:.5f}\".format(objective))\n",
    "          print(\"Min Loss: {0:.5f}\".format(min_value))\n",
    "          print(\"Iter[{}] | w[{}]: Patience added: {}\"\\\n",
    "                .format(t, current_w, patience))\n",
    "          print(\"------------------------------------------------\")\n",
    "    \n",
    "    min_objective[current_w] = min_value\n",
    "    print(\"#################################################################\")\n",
    "    print(\"Training end in global step {}\".format(global_steps))\n",
    "    print(\"Minimum objective: {0:.7f}\".format(min_objective[current_w]))\n",
    "    print(\"[{}] end.\".format(current_w))\n",
    "    print(\"#################################################################\")\n",
    "\n",
    "\n",
    "   \n",
    "  if min_objective[\"h_prime\"] >= min_objective[\"h\"]:\n",
    "    h_t = h\n",
    "  else:\n",
    "    h_t = h_prime\n",
    "  \n",
    "  weight_star = torch.Tensor(h_t.weight)\n",
    "  if weight_trained == 0:\n",
    "    weight_total = torch.add(weight_star, weight_trained)\n",
    "  else:\n",
    "    if weight_star.shape == weight_trained.shape:\n",
    "      weight_total = torch.add(weight_star, weight_trained)\n",
    "    else:\n",
    "      zero_pad_size = weight_star.shape[0] - weight_trained.shape[0]\n",
    "      weight_trained = F.pad(\n",
    "        weight_trained, (0, zero_pad_size), 'constant', 0)\n",
    "      print(\"trained_weight\", weight_trained)\n",
    "      weight_total = torch.add(weight_star, weight_trained)\n",
    "  \n",
    "  val_inputs, val_labels = next(iter(valloader))\n",
    "  _label = torch.zeros([VAL_BATCH_SIZE, NUM_CLASSES], dtype=torch.float32)\n",
    "  _label[range(_label.shape[0]), val_labels] = 1\n",
    "  val_labels = _label\n",
    "  \n",
    "  f_total = f_prev.load_state_dict({'weight': weight_total})\n",
    "  f_total_logits = f_total(val_inputs)\n",
    "  objective_total =  criterion(\n",
    "    trained_logits=f_total_logits, labels=val_labels, mode=\"eval\")\n",
    "  \n",
    "  f_trained = f_prev \n",
    "  f_trained_logits = f_trained(val_inputs)\n",
    "  objective_trained = criterion(\n",
    "    trained_logits=f_trained_logits, labels=val_labels, mode=\"eval\")\n",
    "  \n",
    "  if objective_trained >= objective_total:\n",
    "    # layer 1개 더 있는 모델로 수정\n",
    "    f_t = h_t #h_t는 위에서 이전 모델의 parameter를 제외한 weight및 추가 layer에 대한 parameter만을 학습\n",
    "  else:\n",
    "    #직전 iter에서 학습 완료되어 선정된 모델 그대로\n",
    "    f_t = f_trained\n",
    "    \n",
    "  del val_inputs, val_labels, _label, f_total, f_trained\n",
    "  # Save trained h_t\n",
    "  torch.save({\n",
    "    'iter': t,\n",
    "    'h_or_hprime': current_w,\n",
    "    'model_state_dict': f_t.state_dict(),\n",
    "    'min_objective': min_objective[current_w]},\n",
    "    f=base_path + \"/{}_checkpoint.pt\".format(t))\n",
    "  \n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
