{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-28T12:46:53.318987Z",
     "start_time": "2019-11-28T12:46:52.854694Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import torch.distributions as dist\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-28T12:46:53.327271Z",
     "start_time": "2019-11-28T12:46:53.322447Z"
    }
   },
   "outputs": [],
   "source": [
    "base_path = os.environ[\"HOME\"] + \"/cifar100/adanet\"\n",
    "if os.path.isdir(base_path):\n",
    "  pass\n",
    "else:\n",
    "  os.makedirs(base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-28T12:46:53.338237Z",
     "start_time": "2019-11-28T12:46:53.331879Z"
    }
   },
   "outputs": [],
   "source": [
    "MAX_ITER = 3\n",
    "MAX_EPOCH = 100\n",
    "NUM_CLASSES = 100\n",
    "TRAIN_BATCH_SIZE = 10\n",
    "VAL_BATCH_SIZE = 500\n",
    "TEST_BATCH_SIZE = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-28T12:46:56.531360Z",
     "start_time": "2019-11-28T12:46:53.340714Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "base_path = os.environ['HOME'] + '/cifar100/adanet'\n",
    "if os.path.isdir(base_path):\n",
    "  pass\n",
    "else:\n",
    "  os.makedirs(base_path)\n",
    "\n",
    "writer = SummaryWriter(log_dir=base_path + '/runs/notebook_test')\n",
    "transform = transforms.Compose(\n",
    "    [transforms.Pad(padding=(2, 2, 2, 2)), \n",
    "     transforms.RandomCrop(size=32),\n",
    "     torchvision.transforms.RandomHorizontalFlip(p=0.5),\n",
    "     torchvision.transforms.Resize(size=[224, 224]),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize(\n",
    "       mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))])\n",
    "\n",
    "train = torchvision.datasets.CIFAR100(\n",
    "  root=base_path + '/data', train=True, download=True, transform=transform) \n",
    "test = torchvision.datasets.CIFAR100(\n",
    "  root=base_path + '/data', train=False, download=True, transform=transform) \n",
    "\n",
    "trainlist = torch.utils.data.random_split(train, [40000, 10000])\n",
    "train, val = trainlist[0], trainlist[1]\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "  train, batch_size=TRAIN_BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "valloader = torch.utils.data.DataLoader(\n",
    "  val, batch_size=VAL_BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "  test, batch_size=TEST_BATCH_SIZE, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-28T12:46:56.535990Z",
     "start_time": "2019-11-28T12:46:56.533418Z"
    }
   },
   "outputs": [],
   "source": [
    "# hyperparameter\n",
    "\n",
    "# 최소단위의 block내부에서 layer들을 연결하는 weight값의 constraint\n",
    "P = 2\n",
    "INNER_LAMBDA = 0.1 # Non-negative "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-28T12:46:56.540979Z",
     "start_time": "2019-11-28T12:46:56.538721Z"
    }
   },
   "outputs": [],
   "source": [
    "# 일단 sum weight constraint는 나중에 적용하는 것으로\n",
    "# approximate하게 x 1/num_layer로 하자"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ADANET(S=((x_i, y_i)_{i=1}^{m})$)<br>\n",
    "$f_0 \\leftarrow 0$<br>\n",
    "$for\\ t \\leftarrow 1\\ to\\ T\\ do$<br>\n",
    "$\\;\\;\\;\\;\\;\\; h, h^{'} \\leftarrow WeakLearner(S, f_{t-1})$<br>\n",
    "$\\;\\;\\;\\;\\;\\; w \\leftarrow minimize(F_t(w, h))$<br>\n",
    "$\\;\\;\\;\\;\\;\\; w^{'} \\leftarrow minimize(F_t(w, h^{'}))$<br>\n",
    "$\\;\\;\\;\\;\\;\\; if \\; F_t(w, h) \\le F_t(w, h^{'}) \\; then $<br>\n",
    "$\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\; h_t \\leftarrow h$<br>\n",
    "$\\;\\;\\;\\;\\;\\; else \\;\\; h_t \\leftarrow h^{'}$<br>\n",
    "$\\;\\;\\;\\;\\;\\; if \\; F(w_{t-1}+w^{*}) < F(w_{t-1}) \\;\\; then $<br>\n",
    "$\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\; f_t\\leftarrow f_{t-1}+w^{*}\\cdot h_t$<br>\n",
    "$\\;\\;\\;\\;\\;\\; else \\;\\; return \\;\\; f_{t-1}$<br>\n",
    "$return \\;\\; f_T$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 1st iteration\n",
    "    - weaklearner를 바탕으로 $h_0, h_1$을 만든다. $h_0$은 weaklearner의 ㅛㅐ\n",
    "    - $output = w \\cdot \\mathcal{H_1}$\n",
    "    - $\\sum_{k=1}^{1}||w_k||_1 = 1$\n",
    "    - $\\mathcal{H_1} = u_1 \\cdot \\Psi(x)$ \n",
    "        - Psi(x)는 feature vector를 의미 \n",
    "        - u_1는 AdaNet Layer의 첫번째 inner weight를 의미\n",
    "2. 2nd iter's output vector\n",
    "    - $output = \\sum_{k=1}^{2}w_k \\cdot \\mathcal{H}_k$  1st iter와 동일한 shape의 output\n",
    "    - 다만 차이가 있다면 ensemble 형태로 weighted H_1, H_2의 합계가 최종 output이 됨\n",
    "    - <U>**1st iter에서 학습된 weight들 (ex. $H_1$의 weight와 bias들)은 추가 학습 없이 사용됨**</U>\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-28T12:46:56.553858Z",
     "start_time": "2019-11-28T12:46:56.544051Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# First iter 이후에는 사실상.. 껍데기 역할을 주로 하게 될 것\n",
    "# 그 이후에는 사실상의 fine tuning개념임 \n",
    "class AdaNet(nn.Module):\n",
    "  \n",
    "  def __init__(self, \n",
    "               num_layers, \n",
    "               module_instance, \n",
    "               output_instance):\n",
    "    super(AdaNet, self).__init__()\n",
    "    self.NUM_LAYERS = num_layers\n",
    "    module = [module_instance for i in range(1, num_layers)]\n",
    "    output = [output_instance for i in range(num_layers)]\n",
    "    weight = torch.ones(num_layers) / num_layers\n",
    "    self.weight = nn.Parameter(data=weight, requires_grad=True)\n",
    "    \n",
    "    self.modules_list = nn.ModuleList(module)\n",
    "    self.outputs_list = nn.ModuleList(output) \n",
    "    \"\"\"\n",
    "    output instance들은 일반적으로는 Linear를 쓸것이고 해당 Linear의 weight값들이 \n",
    "    논문에서의 u값을 의미 (l_p constraint가 적용된다.)\n",
    "    \"\"\"\n",
    "    \n",
    "  def forward(self, x):\n",
    "    output = []\n",
    "    for i in range(self.NUM_LAYERS):\n",
    "      if i == 0:\n",
    "        _output = self.outputs_list[0](x)        \n",
    "        output.append(_output)\n",
    "      else:\n",
    "        x = self.modules_list[i - 1](x) \n",
    "        _output = self.outputs_list[i](x) \n",
    "        output.append(_output)\n",
    "    output = torch.stack(output, dim=1)\n",
    "    output = torch.matmul(self.weight, output)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- base classifier는 기본적으로 Residual Block을 이용\n",
    "    -  Input - Output size가 동일하게 \n",
    "- output에서는 기본적으로 global average pooling 적용 및 Dense layer 적용 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-28T12:46:56.569406Z",
     "start_time": "2019-11-28T12:46:56.556920Z"
    }
   },
   "outputs": [],
   "source": [
    "class BaseClassifier(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(BaseClassifier, self).__init__()\n",
    "    self.conv_origin = nn.Conv2d(3, 64, (3, 3), padding=1)\n",
    "    self.conv1 = nn.Conv2d(64, 64, (3, 3), padding=1)\n",
    "    self.conv2 = nn.Conv2d(64, 64, (3, 3), padding=1)\n",
    "    self.batchnorm1 = nn.BatchNorm2d(num_features=64)\n",
    "    self.batchnorm2 = nn.BatchNorm2d(num_features=64)\n",
    "    \n",
    "  def forward(self, x):\n",
    "    if x.shape[1] == 3:\n",
    "      origin_x = self.conv_origin(x)\n",
    "      x = self.conv1(origin_x)\n",
    "      x = self.batchnorm1(x)\n",
    "      x = F.relu(x)\n",
    "      x = self.conv2(x)\n",
    "      x = self.batchnorm2(x)\n",
    "      x = F.relu(x)\n",
    "      x = torch.add(origin_x, x)\n",
    "    elif x.shape[1] == 64:\n",
    "      origin_x = x\n",
    "      x = self.conv1(x)\n",
    "      x = self.batchnorm1(x)\n",
    "      x = F.relu(x)\n",
    "      x = self.conv2(x)\n",
    "      x = self.batchnorm2(x)\n",
    "      x = F.relu(x)\n",
    "      x = torch.add(origin_x, x)\n",
    "    return x\n",
    "\n",
    "  \n",
    "class OutputModule(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(OutputModule, self).__init__()\n",
    "    self.globalavgpool = nn.AvgPool2d(kernel_size=(224, 224))\n",
    "    self.fc1 = nn.Linear(3, 100)\n",
    "    self.fc2 = nn.Linear(64, 100)\n",
    "    \n",
    "  def forward(self, x):\n",
    "    if x.shape[1] == 3:\n",
    "      x = self.globalavgpool(x).view(-1, 3)\n",
    "      logit = self.fc1(x)\n",
    "    elif x.shape[1] == 64:\n",
    "      x = self.globalavgpool(x).view(-1, 64)\n",
    "      logit = self.fc2(x)\n",
    "    return logit\n",
    "  \n",
    "# training시 running memory가 어떨지 고려하도록"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-28T12:46:56.576061Z",
     "start_time": "2019-11-28T12:46:56.571525Z"
    }
   },
   "outputs": [],
   "source": [
    "# loss function \n",
    "def criterion(trained_logits, labels, \n",
    "              mode, penalize_term=torch.tensor(0), \n",
    "              training_logits=None, device=torch.device('cpu')):\n",
    "  \n",
    "  trained_logtis = trained_logits.to(device)\n",
    "  labels = labels.to(device)\n",
    "  penalize_term = penalize_term.to(device)\n",
    "  \n",
    "  if mode == \"train\":\n",
    "    training_logits = training_logits.to(device) \n",
    "    y_f = torch.mul(labels, trained_logits)\n",
    "    y_wu = torch.mul(labels, training_logits)\n",
    "    \n",
    "    # surrogate loss\n",
    "    loss = torch.exp(1 - y_f - y_wu)\n",
    "    loss = torch.mean(loss) + penalize_term\n",
    "\n",
    "  elif mode == \"eval\":\n",
    "    y_f = torch.mul(labels, trained_logits)\n",
    "    loss = torch.exp(1 - y_f)\n",
    "    loss = torch.mean(loss)\n",
    "  else:\n",
    "    raise Exception(\"Putting the right 'mode' argument.\")\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-28T12:46:56.579709Z",
     "start_time": "2019-11-28T12:46:56.577477Z"
    }
   },
   "outputs": [],
   "source": [
    "max_patience = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-11-28T12:46:53.107Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([0.5000, 0.5000], device='cuda:0', requires_grad=True)\n",
      "#------------------------------------------------------------#\n",
      "Start h\n",
      "**** ITERATION [1] ****\n",
      "Learning ** h **\n",
      "EPOCH [1] | GLOBAL STEP [100]\n",
      "Loss: 2.71850681\n",
      "Min Loss: 2.70953178\n",
      "Iter[1] | w[h]: Patience added: 92\n",
      "------------------------------------------------\n",
      "**** ITERATION [1] ****\n",
      "Learning ** h **\n",
      "EPOCH [1] | GLOBAL STEP [200]\n",
      "Loss: 2.71531487\n",
      "Min Loss: 2.70953178\n",
      "Iter[1] | w[h]: Patience added: 192\n",
      "------------------------------------------------\n",
      "#################################################################\n",
      "Training end in global step 308\n",
      "Minimum objective: 2.70953178\n",
      "[h] end.\n",
      "#################################################################\n",
      "Parameter containing:\n",
      "tensor([0.9120], device='cuda:0', requires_grad=True)\n",
      "#------------------------------------------------------------#\n",
      "Start h_prime\n",
      "**** ITERATION [1] ****\n",
      "Learning ** h_prime **\n",
      "EPOCH [1] | GLOBAL STEP [100]\n",
      "Loss: 2.70428586\n",
      "Min Loss: 2.70422006\n",
      "Iter[1] | w[h_prime]: Patience added: 81\n",
      "------------------------------------------------\n",
      "**** ITERATION [1] ****\n",
      "Learning ** h_prime **\n",
      "EPOCH [1] | GLOBAL STEP [200]\n",
      "Loss: 2.70137215\n",
      "Min Loss: 2.69570327\n",
      "Iter[1] | w[h_prime]: Patience added: 167\n",
      "------------------------------------------------\n",
      "#################################################################\n",
      "Training end in global step 336\n",
      "Minimum objective: 2.69433451\n",
      "[h_prime] end.\n",
      "#################################################################\n",
      "Parameter containing:\n",
      "tensor([0.5796, 0.7548], device='cuda:0', requires_grad=True)\n",
      "Eval h and h_prime\n",
      "Parameter containing:\n",
      "tensor([0.5796, 0.7548], requires_grad=True)\n",
      "weight_star Parameter containing:\n",
      "tensor([0.5796, 0.7548], requires_grad=True)\n",
      "weight total tensor([0.5796, 0.7548], grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'f_trained_logits' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-0beef712803d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    192\u001b[0m     trained_logits=f_total_logits, labels=val_labels, mode=\"eval\", device=torch.device('cpu'))\n\u001b[1;32m    193\u001b[0m   objective_trained = criterion(\n\u001b[0;32m--> 194\u001b[0;31m     trained_logits=f_trained_logits, labels=val_labels, mode=\"eval\", device=torch.device('cpu'))\n\u001b[0m\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m   \"\"\"\n",
      "\u001b[0;31mNameError\u001b[0m: name 'f_trained_logits' is not defined"
     ]
    }
   ],
   "source": [
    "base_module = BaseClassifier()\n",
    "out_module = OutputModule()\n",
    "\n",
    "\n",
    "for t in range(1, MAX_ITER + 1): \n",
    "  # range(1, max_iter + 1) for convenience\n",
    "  h = AdaNet(t, base_module, out_module)\n",
    "  h_prime = AdaNet(t + 1, base_module, out_module)\n",
    "  \n",
    "  h = h.to(device)\n",
    "  h_prime = h_prime.to(device)\n",
    "  \n",
    "  weaklearners = [h, h_prime]\n",
    "  \n",
    "  # container for minimized objective value\n",
    "  min_objective = {\"h\": 0., \"h_prime\": 0.}\n",
    "  \n",
    "  print(weaklearners[1].weight)\n",
    "  \n",
    "  for w in range(2): \n",
    "    # if w = 1 then h and otherwise h'\n",
    "    weaklearner = weaklearners[w]\n",
    "    if t == 1:\n",
    "      logit_trained = 0\n",
    "      weight_trained = 0\n",
    "    else:\n",
    "      ckpt_path = base_path + \"/{}_checkpoint.pt\".format(t - 1)\n",
    "      checkpoint = torch.load(ckpt_path)\n",
    "      print(\"Load {}\".format(ckpt_path))\n",
    "\n",
    "      prev_params = [param for param in checkpoint['model_state_dict']]\n",
    "      prev_weight_dict = {\n",
    "        k: v for k, v in checkpoint['model_state_dict'].items()\n",
    "        if k == 'weight'}\n",
    "\n",
    "      param_locate = [param for param in weaklearner.state_dict()]\n",
    "      prev_param_index = [param_locate.index(i) for i in prev_params]\n",
    "      prev_param_index.remove(0) # Weight parameter have to train \n",
    "      \n",
    "      idx = 0\n",
    "      for param in weaklearner.parameters():\n",
    "        if idx in prev_trained_param_idx:\n",
    "          param.requires_grad = False\n",
    "        else:\n",
    "          param.requires_grad = True\n",
    "        idx += 1\n",
    "    \n",
    "    optimizer = optim.Adam(params=weaklearner.parameters())\n",
    "    current_w = \"h\" if w == 0 else \"h_prime\"\n",
    "    print(\"#------------------------------------------------------------#\")\n",
    "    print(\"Start {}\".format(current_w))\n",
    "    early_metrics = []\n",
    "    min_value = 0.\n",
    "    patience = 0\n",
    "    global_steps = 0\n",
    "    \n",
    "    for epoch in range(MAX_EPOCH):\n",
    "      \n",
    "      \"\"\"\n",
    "      if t == 1:\n",
    "        logit_trained = 0\n",
    "        weight_trained = 0\n",
    "      else: \n",
    "        ckpt_path = base_path + \"/{}_checkpoint.pt\".format(t - 1)\n",
    "        checkpoint = torch.load(ckpt_path)\n",
    "        print(\"Load {}\".format(ckpt_path))\n",
    "\n",
    "        prev_weight_dict = {\n",
    "          k: v for k, v in checkpoint['model_state_dict'].items()\n",
    "          if k == 'weight'}\n",
    "        \n",
    "        # Load previous iteration function \n",
    "        # NOTE: 이전 iter에서 학습된 f_{t-1}에서 나온 logit\n",
    "        f_prev = AdaNet(t - 1, base_module, out_module)\n",
    "        f_prev.load_state_dict(checkpoint['model_state_dict'])\n",
    "        logit_trained = h_prev(inputs)\n",
    "\n",
    "        weight_trained = checkpoint['model_state_dict']['weight']\n",
    "        \"\"\"\n",
    "      if t == 1:\n",
    "        logit_trained = torch.tensor(0.).to(device)\n",
    "        weight_trained = torch.tensor(0.).to(torch.device('cpu'))\n",
    "      else:\n",
    "        f_prev = AdaNet(t - 1, base_module, out_module)\n",
    "        f_prev = f_prev.to(torch.device('cpu'))\n",
    "        f_prev.load_state_dict(checkpoint['model_state_dict'], strict=False)\n",
    "        logit_trained = h_prev(inputs).to(device)\n",
    "        weight_trained = checkpoint['model_state_dict']['weight'].to(torch.device('cpu'))\n",
    "\n",
    "      for i, data in enumerate(trainloader):\n",
    "        global_steps += 1\n",
    "        if patience >= max_patience:\n",
    "          break  \n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        _label = torch.zeros(\n",
    "          [TRAIN_BATCH_SIZE, NUM_CLASSES], dtype=torch.float32)\n",
    "        _label[range(_label.shape[0]), labels] = 1\n",
    "        labels = _label\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        ######\n",
    "        #weaklearner = weaklearner\n",
    "        ######\n",
    "        logits = weaklearner(inputs)\n",
    "        #print(\"training logit\", logits)\n",
    "        objective = criterion(\n",
    "          trained_logits=logit_trained, \n",
    "          training_logits=logits,\n",
    "          labels=labels, mode=\"train\", device=device)\n",
    "        objective.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        del inputs, labels, data, logits, _label\n",
    "        \n",
    "        \n",
    "        ##################\n",
    "        # Early stopping #  early stop checking for every global steps.\n",
    "        ##################\n",
    "        _metric = objective.item()\n",
    "        if len(early_metrics) < 2:\n",
    "          early_metrics.append(_metric)\n",
    "        elif len(early_metrics) >= 2:\n",
    "          if _metric > min_value:\n",
    "            patience += 1\n",
    "          early_metrics.append(_metric)\n",
    "          min_value = min(early_metrics)\n",
    "          early_metrics.sort()\n",
    "          early_metrics = early_metrics[:2]\n",
    "\n",
    "        if i % 100 == 99:\n",
    "          print(\"**** ITERATION [{}] ****\".format(t))\n",
    "          if w == 0: \n",
    "            print(\"Learning ** h **\")\n",
    "          else:\n",
    "            print(\"Learning ** h_prime **\")\n",
    "          print(\n",
    "            \"EPOCH [{}] | GLOBAL STEP [{}]\".format(epoch + 1, global_steps))\n",
    "          print(\"Loss: {0:.8f}\".format(objective))\n",
    "          print(\"Min Loss: {0:.8f}\".format(min_value))\n",
    "          print(\"Iter[{}] | w[{}]: Patience added: {}\"\\\n",
    "                .format(t, current_w, patience))\n",
    "          print(\"------------------------------------------------\")\n",
    "    \n",
    "    min_objective[current_w] = min_value\n",
    "    print(\"#################################################################\")\n",
    "    print(\"Training end in global step {}\".format(global_steps))\n",
    "    print(\"Minimum objective: {0:.8f}\".format(min_objective[current_w]))\n",
    "    print(\"[{}] end.\".format(current_w))\n",
    "    print(\"#################################################################\")\n",
    "    print(weaklearner.weight)\n",
    "\n",
    "\n",
    "  print(\"Eval h and h_prime\")\n",
    "  if min_objective[\"h_prime\"] >= min_objective[\"h\"]:\n",
    "    h_t = h\n",
    "  else:\n",
    "    h_t = h_prime\n",
    "  \n",
    "  \n",
    "  h_t = h_t.to(torch.device('cpu'))\n",
    "  print(h_t.weight)\n",
    "  #weight_star = torch.Tensor(h_t.weight)\n",
    "  weight_star = h_t.weight\n",
    "  print(\"weight_star\", weight_star)\n",
    "  if weight_trained == 0:\n",
    "    weight_total = torch.add(weight_star, weight_trained)\n",
    "  else:\n",
    "    if weight_star.shape == weight_trained.shape:\n",
    "      weight_total = torch.add(weight_star, weight_trained)\n",
    "    else:\n",
    "      zero_pad_size = weight_star.shape[0] - weight_trained.shape[0]\n",
    "      weight_trained = F.pad(\n",
    "        weight_trained, (0, zero_pad_size), 'constant', 0)\n",
    "      print(\"trained_weight\", weight_trained)\n",
    "      weight_total = torch.add(weight_star, weight_trained)\n",
    "  print(\"weight total\", weight_total)\n",
    "  \n",
    "  val_inputs, val_labels = next(iter(valloader))\n",
    "  _label = torch.zeros([VAL_BATCH_SIZE, NUM_CLASSES], dtype=torch.float32)\n",
    "  _label[range(_label.shape[0]), val_labels] = 1\n",
    "  val_labels = _label\n",
    "  if t == 1:\n",
    "    f_total_logits = torch.tensor(0.)\n",
    "  else:\n",
    "    f_total = f_prev.load_state_dict({'weight': weight_total})\n",
    "    f_trained = f_prev \n",
    "    f_trained_logits = f_trained(val_inputs)\n",
    "    f_total_logits = f_total(val_inputs)\n",
    "    \n",
    "  objective_total =  criterion(\n",
    "    trained_logits=f_total_logits, labels=val_labels, mode=\"eval\", device=torch.device('cpu'))\n",
    "  objective_trained = criterion(\n",
    "    trained_logits=f_trained_logits, labels=val_labels, mode=\"eval\", device=torch.device('cpu'))\n",
    "  \n",
    "  \"\"\"\n",
    "  f_total = f_prev.load_state_dict({'weight': weight_total})\n",
    "  f_total_logits = f_total(val_inputs)\n",
    "  objective_total =  criterion(\n",
    "    trained_logits=f_total_logits, labels=val_labels, mode=\"eval\", device=torch.device('cpu'))\n",
    "  \n",
    "  f_trained = f_prev \n",
    "  f_trained_logits = f_trained(val_inputs)\n",
    "  objective_trained = criterion(\n",
    "    trained_logits=f_trained_logits, labels=val_labels, mode=\"eval\", device=torch.device('cpu'))\n",
    "  \"\"\"\n",
    "  print(\"objective_totla\", objective_total)\n",
    "  print('objective_trained', objective_trained)\n",
    "  \n",
    "  if objective_trained >= objective_total:\n",
    "    # layer 1개 더 있는 모델로 수정\n",
    "    f_t = h_t #h_t는 위에서 이전 모델의 parameter를 제외한 weight및 추가 layer에 대한 parameter만을 학습\n",
    "  else:\n",
    "    #직전 iter에서 학습 완료되어 선정된 모델 그대로\n",
    "    f_t = f_trained\n",
    "    \n",
    "  del val_inputs, val_labels, _label, f_total, f_trained\n",
    "  # Save trained h_t\n",
    "  torch.save({\n",
    "    'iter': t,\n",
    "    'h_or_hprime': current_w,\n",
    "    'model_state_dict': f_t.state_dict(),\n",
    "    'min_objective': min_objective[current_w]},\n",
    "    f=base_path + \"/{}_checkpoint.pt\".format(t))\n",
    "  \n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
