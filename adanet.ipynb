{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-28T12:46:53.318987Z",
     "start_time": "2019-11-28T12:46:52.854694Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import torch.distributions as dist\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-28T12:46:53.327271Z",
     "start_time": "2019-11-28T12:46:53.322447Z"
    }
   },
   "outputs": [],
   "source": [
    "base_path = os.environ[\"HOME\"] + \"/cifar100/adanet\"\n",
    "if os.path.isdir(base_path):\n",
    "  pass\n",
    "else:\n",
    "  os.makedirs(base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-28T12:46:53.338237Z",
     "start_time": "2019-11-28T12:46:53.331879Z"
    }
   },
   "outputs": [],
   "source": [
    "MAX_ITER = 50\n",
    "MAX_EPOCH = 1000\n",
    "NUM_CLASSES = 100\n",
    "TRAIN_BATCH_SIZE = 10\n",
    "VAL_BATCH_SIZE = 10\n",
    "TEST_BATCH_SIZE = 100\n",
    "MAX_PATIENCE = 100\n",
    "THRESHOLD = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-28T12:46:56.531360Z",
     "start_time": "2019-11-28T12:46:53.340714Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "base_path = os.environ['HOME'] + '/cifar100/adanet'\n",
    "if os.path.isdir(base_path):\n",
    "  pass\n",
    "else:\n",
    "  os.makedirs(base_path)\n",
    "\n",
    "writer = SummaryWriter(log_dir=base_path + '/runs/notebook_test')\n",
    "transform = transforms.Compose(\n",
    "    [transforms.Pad(padding=(2, 2, 2, 2)), \n",
    "     transforms.RandomCrop(size=32),\n",
    "     torchvision.transforms.RandomHorizontalFlip(p=0.5),\n",
    "     torchvision.transforms.Resize(size=[224, 224]),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize(\n",
    "       mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))])\n",
    "\n",
    "train = torchvision.datasets.CIFAR100(\n",
    "  root=base_path + '/data', train=True, download=True, transform=transform) \n",
    "test = torchvision.datasets.CIFAR100(\n",
    "  root=base_path + '/data', train=False, download=True, transform=transform) \n",
    "\n",
    "trainlist = torch.utils.data.random_split(train, [40000, 10000])\n",
    "train, val = trainlist[0], trainlist[1]\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "  train, batch_size=TRAIN_BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "valloader = torch.utils.data.DataLoader(\n",
    "  val, batch_size=VAL_BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "  test, batch_size=TEST_BATCH_SIZE, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-28T12:46:56.535990Z",
     "start_time": "2019-11-28T12:46:56.533418Z"
    }
   },
   "outputs": [],
   "source": [
    "# hyperparameter\n",
    "# 최소단위의 block내부에서 layer들을 연결하는 weight값의 constraint\n",
    "P = 2\n",
    "UPPER_LAMBDA = 0.1 # Non-negative\n",
    "divQ = 1 - (1 / P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-28T12:46:56.540979Z",
     "start_time": "2019-11-28T12:46:56.538721Z"
    }
   },
   "outputs": [],
   "source": [
    "# 일단 sum weight constraint는 나중에 적용하는 것으로\n",
    "# approximate하게 x 1/num_layer로 하자"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ADANET(S=((x_i, y_i)_{i=1}^{m})$)<br>\n",
    "$f_0 \\leftarrow 0$<br>\n",
    "$for\\ t \\leftarrow 1\\ to\\ T\\ do$<br>\n",
    "$\\;\\;\\;\\;\\;\\; h, h^{'} \\leftarrow WeakLearner(S, f_{t-1})$<br>\n",
    "$\\;\\;\\;\\;\\;\\; w \\leftarrow minimize(F_t(w, h))$<br>\n",
    "$\\;\\;\\;\\;\\;\\; w^{'} \\leftarrow minimize(F_t(w, h^{'}))$<br>\n",
    "$\\;\\;\\;\\;\\;\\; if \\; F_t(w, h) \\le F_t(w, h^{'}) \\; then $<br>\n",
    "$\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\; h_t \\leftarrow h$<br>\n",
    "$\\;\\;\\;\\;\\;\\; else \\;\\; h_t \\leftarrow h^{'}$<br>\n",
    "$\\;\\;\\;\\;\\;\\; if \\; F(w_{t-1}+w^{*}) < F(w_{t-1}) \\;\\; then $<br>\n",
    "$\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\; f_t\\leftarrow f_{t-1}+w^{*}\\cdot h_t$<br>\n",
    "$\\;\\;\\;\\;\\;\\; else \\;\\; return \\;\\; f_{t-1}$<br>\n",
    "$return \\;\\; f_T$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 1st iteration\n",
    "    - weaklearner를 바탕으로 $h_0, h_1$을 만든다. $h_0$은 weaklearner의 ㅛㅐ\n",
    "    - $output = w \\cdot \\mathcal{H_1}$\n",
    "    - $\\sum_{k=1}^{1}||w_k||_1 = 1$\n",
    "    - $\\mathcal{H_1} = u_1 \\cdot \\Psi(x)$ \n",
    "        - Psi(x)는 feature vector를 의미 \n",
    "        - u_1는 AdaNet Layer의 첫번째 inner weight를 의미\n",
    "2. 2nd iter's output vector\n",
    "    - $output = \\sum_{k=1}^{2}w_k \\cdot \\mathcal{H}_k$  1st iter와 동일한 shape의 output\n",
    "    - 다만 차이가 있다면 ensemble 형태로 weighted H_1, H_2의 합계가 최종 output이 됨\n",
    "    - <U>**1st iter에서 학습된 weight들 (ex. $H_1$의 weight와 bias들)은 추가 학습 없이 사용됨**</U>\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Empirical Rademacher complexity can be upper bounded as: <br>\n",
    "$\\Lambda_k = \\prod^k_{s=1} 2\\Lambda_{s, s-1}$  - Base classifier's weight <br>\n",
    "$N_k = \\prod^k_{s=1}n_{s-1}$ - Base classifier's each layer features <br>\n",
    "$r_{\\infty} = max_{i\\in[1,m]}\\| \\Psi(x_i) \\|_\\infty$ - Input feature's infinity norm <br>\n",
    "$\\mathfrak{R}(\\mathcal{H}) \\leq r_{\\infty}\\Lambda_k N_k^{\\frac{1}{q}}\\sqrt{\\frac{log(2n_0)}{2m}}$ - Rademacher complexity's upper bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-28T12:46:56.553858Z",
     "start_time": "2019-11-28T12:46:56.544051Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class AdaNet(nn.Module):\n",
    "  \n",
    "  def __init__(self, \n",
    "               num_layers, \n",
    "               module_instance, \n",
    "               output_instance):\n",
    "    super(AdaNet, self).__init__()\n",
    "    self.NUM_LAYERS = num_layers\n",
    "    module = [module_instance for i in range(1, num_layers)]\n",
    "    output = [output_instance for i in range(num_layers)]\n",
    "    weight = torch.ones(num_layers) / num_layers # To make it simple. I just used mean normalize.\n",
    "    self.weight = nn.Parameter(data=weight, requires_grad=True)\n",
    "    \n",
    "    self.modules_list = nn.ModuleList(module)\n",
    "    self.outputs_list = nn.ModuleList(output) \n",
    "    self.softmax = nn.Softmax(dim=0)\n",
    "    \n",
    "  def forward(self, x):\n",
    "    output = []\n",
    "    for i in range(self.NUM_LAYERS):\n",
    "      if i == 0:\n",
    "        _output = self.outputs_list[0](x)        \n",
    "        output.append(_output)\n",
    "      else:\n",
    "        x = self.modules_list[i - 1](x) \n",
    "        _output = self.outputs_list[i](x) \n",
    "        output.append(_output)\n",
    "    output = torch.stack(output, dim=1)\n",
    "    output = torch.matmul(self.softmax(self.weight), output)      \n",
    "    # Approximates Rademacher complexity as the square-root of the depth.\n",
    "    # Reference : https://github.com/tensorflow/adanet/blob/master/adanet/examples/tutorials/adanet_objective.ipynb\n",
    "    rademacher_complexity = torch.sqrt(torch.tensor(self.NUM_LAYERS, dtype=torch.float32))\n",
    "    return output, rademacher_complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-28T12:46:56.569406Z",
     "start_time": "2019-11-28T12:46:56.556920Z"
    }
   },
   "outputs": [],
   "source": [
    "class BaseClassifier(nn.Module):\n",
    "  def __init__(self, upper_lambda=1, p=2):\n",
    "    super(BaseClassifier, self).__init__()\n",
    "    self.conv_origin = nn.Conv2d(3, 64, (3, 3), padding=1)\n",
    "    self.conv_origin.weight = nn.Parameter(\n",
    "      self.conv_origin.weight / (torch.norm(self.conv_origin.weight, p=p)) * upper_lambda,\n",
    "      requires_grad=True)\n",
    "    self.conv1 = nn.Conv2d(64, 64, (3, 3), padding=1)\n",
    "    self.conv1.weight = nn.Parameter(\n",
    "      self.conv1.weight / (torch.norm(self.conv1.weight, p=p)) * upper_lambda,\n",
    "      requires_grad=True)\n",
    "    self.conv2 = nn.Conv2d(64, 64, (3, 3), padding=1)\n",
    "    self.conv2.weight = nn.Parameter(\n",
    "      self.conv2.weight / (torch.norm(self.conv2.weight, p=p)) * upper_lambda,\n",
    "      requires_grad=True)\n",
    "    self.batchnorm1 = nn.BatchNorm2d(num_features=64)\n",
    "    self.batchnorm2 = nn.BatchNorm2d(num_features=64)\n",
    "    \n",
    "    \n",
    "  def forward(self, x):\n",
    "    if x.shape[1] == 3:\n",
    "      origin_x = self.conv_origin(x)\n",
    "      x = self.conv1(origin_x)\n",
    "      x = self.batchnorm1(x)\n",
    "      x = F.relu(x)\n",
    "      x = self.conv2(x)\n",
    "      x = self.batchnorm2(x)\n",
    "      x = F.relu(x)\n",
    "      x = torch.add(origin_x, x)\n",
    "    elif x.shape[1] == 64:\n",
    "      origin_x = x\n",
    "      x = self.conv1(x)\n",
    "      x = self.batchnorm1(x)\n",
    "      x = F.relu(x)\n",
    "      x = self.conv2(x)\n",
    "      x = self.batchnorm2(x)\n",
    "      x = F.relu(x)\n",
    "      x = torch.add(origin_x, x)\n",
    "    return x\n",
    "\n",
    "  \n",
    "class OutputModule(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(OutputModule, self).__init__()\n",
    "    self.globalavgpool = nn.AvgPool2d(kernel_size=(224, 224))\n",
    "    self.fc1 = nn.Linear(3, 100)\n",
    "    self.fc2 = nn.Linear(64, 100)\n",
    "    \n",
    "  def forward(self, x):\n",
    "    if x.shape[1] == 3:\n",
    "      x = self.globalavgpool(x).view(-1, 3)\n",
    "      logit = self.fc1(x)\n",
    "    elif x.shape[1] == 64:\n",
    "      x = self.globalavgpool(x).view(-1, 64)\n",
    "      logit = self.fc2(x)\n",
    "    return logit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Consider memory usage for each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-28T12:46:56.576061Z",
     "start_time": "2019-11-28T12:46:56.571525Z"
    }
   },
   "outputs": [],
   "source": [
    "# loss function \n",
    "def criterion(trained_logits, labels, \n",
    "              mode, penalize=torch.tensor(0),\n",
    "              lambda_ = 0.0001, beta = 0.0001,\n",
    "              weight=torch.tensor(0), training_logits=None,\n",
    "              device=torch.device('cpu')):\n",
    "  trained_logtis = trained_logits.to(device)\n",
    "  labels = labels.to(device)\n",
    "  penalize = penalize.to(device)\n",
    "  penalize_term =  torch.mul(\n",
    "    (lambda_ * penalize + beta),\n",
    "    torch.abs(nn.Softmax(dim=0)(weight)))\n",
    "  penalize_term = torch.sum(penalize_term)\n",
    "  if mode == \"train\":\n",
    "    training_logits = training_logits.to(device) \n",
    "    y_f = torch.mul(labels, trained_logits)\n",
    "    y_wu = torch.mul(labels, training_logits)\n",
    "    \n",
    "    penalize_term = torch.sum(penalize_term)\n",
    "    loss = torch.log(torch.tensor(1.) + torch.exp(1 - y_f - y_wu))\n",
    "    loss = torch.mean(loss) + penalize_term \n",
    "\n",
    "  elif mode == \"eval\":\n",
    "    y_f = torch.mul(labels, trained_logits)\n",
    "    loss = torch.log(torch.tensor(1.) + torch.exp(1 - y_f))\n",
    "    loss = torch.mean(loss) + penalize_term\n",
    "  else:\n",
    "    raise Exception(\"Putting the right 'mode' argument.\")\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-11-28T12:46:53.107Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#------------------------------------------------------------#\n",
      "Start h\n",
      "**** ITERATION [1] ****\n",
      "Learning ** h **\n",
      "EPOCH [1] | GLOBAL STEP [100]\n",
      "Running Loss: 1.31356822\n",
      "Loss: 1.31356822\n",
      "Min Loss: 0.00000000\n",
      "Iter[1] | w[h]: Patience added: 0\n",
      "Trained weight tensor([1.], device='cuda:0', grad_fn=<SoftmaxBackward>)\n",
      "------------------------------------------------\n",
      "**** ITERATION [1] ****\n",
      "Learning ** h **\n",
      "EPOCH [1] | GLOBAL STEP [200]\n",
      "Running Loss: 1.31327249\n",
      "Loss: 1.31327249\n",
      "Min Loss: 0.00000000\n",
      "Iter[1] | w[h]: Patience added: 0\n",
      "Trained weight tensor([1.], device='cuda:0', grad_fn=<SoftmaxBackward>)\n",
      "------------------------------------------------\n",
      "**** ITERATION [1] ****\n",
      "Learning ** h **\n",
      "EPOCH [1] | GLOBAL STEP [300]\n",
      "Running Loss: 1.31290531\n",
      "Loss: 1.31290531\n",
      "Min Loss: 1.31290531\n",
      "Iter[1] | w[h]: Patience added: 1\n",
      "Trained weight tensor([1.], device='cuda:0', grad_fn=<SoftmaxBackward>)\n",
      "------------------------------------------------\n",
      "**** ITERATION [1] ****\n",
      "Learning ** h **\n",
      "EPOCH [1] | GLOBAL STEP [400]\n",
      "Running Loss: 1.31278461\n",
      "Loss: 1.31278461\n",
      "Min Loss: 1.31278461\n",
      "Iter[1] | w[h]: Patience added: 1\n",
      "Trained weight tensor([1.], device='cuda:0', grad_fn=<SoftmaxBackward>)\n",
      "------------------------------------------------\n",
      "**** ITERATION [1] ****\n",
      "Learning ** h **\n",
      "EPOCH [1] | GLOBAL STEP [500]\n",
      "Running Loss: 1.31232811\n",
      "Loss: 1.31232811\n",
      "Min Loss: 1.31232811\n",
      "Iter[1] | w[h]: Patience added: 1\n",
      "Trained weight tensor([1.], device='cuda:0', grad_fn=<SoftmaxBackward>)\n",
      "------------------------------------------------\n",
      "**** ITERATION [1] ****\n",
      "Learning ** h **\n",
      "EPOCH [1] | GLOBAL STEP [600]\n",
      "Running Loss: 1.31213622\n",
      "Loss: 1.31213622\n",
      "Min Loss: 1.31213622\n",
      "Iter[1] | w[h]: Patience added: 1\n",
      "Trained weight tensor([1.], device='cuda:0', grad_fn=<SoftmaxBackward>)\n",
      "------------------------------------------------\n",
      "**** ITERATION [1] ****\n",
      "Learning ** h **\n",
      "EPOCH [1] | GLOBAL STEP [700]\n",
      "Running Loss: 1.31187143\n",
      "Loss: 1.31187143\n",
      "Min Loss: 1.31187143\n",
      "Iter[1] | w[h]: Patience added: 1\n",
      "Trained weight tensor([1.], device='cuda:0', grad_fn=<SoftmaxBackward>)\n",
      "------------------------------------------------\n",
      "**** ITERATION [1] ****\n",
      "Learning ** h **\n",
      "EPOCH [1] | GLOBAL STEP [800]\n",
      "Running Loss: 1.31148474\n",
      "Loss: 1.31148474\n",
      "Min Loss: 1.31148474\n",
      "Iter[1] | w[h]: Patience added: 1\n",
      "Trained weight tensor([1.], device='cuda:0', grad_fn=<SoftmaxBackward>)\n",
      "------------------------------------------------\n",
      "**** ITERATION [1] ****\n",
      "Learning ** h **\n",
      "EPOCH [1] | GLOBAL STEP [900]\n",
      "Running Loss: 1.31145905\n",
      "Loss: 1.31145905\n",
      "Min Loss: 1.31145905\n",
      "Iter[1] | w[h]: Patience added: 1\n",
      "Trained weight tensor([1.], device='cuda:0', grad_fn=<SoftmaxBackward>)\n",
      "------------------------------------------------\n",
      "**** ITERATION [1] ****\n",
      "Learning ** h **\n",
      "EPOCH [1] | GLOBAL STEP [1000]\n",
      "Running Loss: 1.31117175\n",
      "Loss: 1.31117175\n",
      "Min Loss: 1.31117175\n",
      "Iter[1] | w[h]: Patience added: 1\n",
      "Trained weight tensor([1.], device='cuda:0', grad_fn=<SoftmaxBackward>)\n",
      "------------------------------------------------\n",
      "**** ITERATION [1] ****\n",
      "Learning ** h **\n",
      "EPOCH [1] | GLOBAL STEP [1100]\n",
      "Running Loss: 1.31081998\n",
      "Loss: 1.31081998\n",
      "Min Loss: 1.31081998\n",
      "Iter[1] | w[h]: Patience added: 1\n",
      "Trained weight tensor([1.], device='cuda:0', grad_fn=<SoftmaxBackward>)\n",
      "------------------------------------------------\n",
      "**** ITERATION [1] ****\n",
      "Learning ** h **\n",
      "EPOCH [1] | GLOBAL STEP [1200]\n",
      "Running Loss: 1.31055200\n",
      "Loss: 1.31055200\n",
      "Min Loss: 1.31055200\n",
      "Iter[1] | w[h]: Patience added: 1\n",
      "Trained weight tensor([1.], device='cuda:0', grad_fn=<SoftmaxBackward>)\n",
      "------------------------------------------------\n",
      "**** ITERATION [1] ****\n",
      "Learning ** h **\n",
      "EPOCH [1] | GLOBAL STEP [1300]\n",
      "Running Loss: 1.31046475\n",
      "Loss: 1.31046475\n",
      "Min Loss: 1.31046475\n",
      "Iter[1] | w[h]: Patience added: 1\n",
      "Trained weight tensor([1.], device='cuda:0', grad_fn=<SoftmaxBackward>)\n",
      "------------------------------------------------\n",
      "**** ITERATION [1] ****\n",
      "Learning ** h **\n",
      "EPOCH [1] | GLOBAL STEP [1400]\n",
      "Running Loss: 1.31026942\n",
      "Loss: 1.31026942\n",
      "Min Loss: 1.31026942\n",
      "Iter[1] | w[h]: Patience added: 1\n",
      "Trained weight tensor([1.], device='cuda:0', grad_fn=<SoftmaxBackward>)\n",
      "------------------------------------------------\n",
      "**** ITERATION [1] ****\n",
      "Learning ** h **\n",
      "EPOCH [1] | GLOBAL STEP [1500]\n",
      "Running Loss: 1.31003502\n",
      "Loss: 1.31003502\n",
      "Min Loss: 1.31003502\n",
      "Iter[1] | w[h]: Patience added: 1\n",
      "Trained weight tensor([1.], device='cuda:0', grad_fn=<SoftmaxBackward>)\n",
      "------------------------------------------------\n",
      "**** ITERATION [1] ****\n",
      "Learning ** h **\n",
      "EPOCH [1] | GLOBAL STEP [1600]\n",
      "Running Loss: 1.30992938\n",
      "Loss: 1.30992938\n",
      "Min Loss: 1.30992938\n",
      "Iter[1] | w[h]: Patience added: 1\n",
      "Trained weight tensor([1.], device='cuda:0', grad_fn=<SoftmaxBackward>)\n",
      "------------------------------------------------\n",
      "**** ITERATION [1] ****\n",
      "Learning ** h **\n",
      "EPOCH [1] | GLOBAL STEP [1700]\n",
      "Running Loss: 1.30956384\n",
      "Loss: 1.30956384\n",
      "Min Loss: 1.30956384\n",
      "Iter[1] | w[h]: Patience added: 1\n",
      "Trained weight tensor([1.], device='cuda:0', grad_fn=<SoftmaxBackward>)\n",
      "------------------------------------------------\n",
      "**** ITERATION [1] ****\n",
      "Learning ** h **\n",
      "EPOCH [1] | GLOBAL STEP [1800]\n",
      "Running Loss: 1.30934852\n",
      "Loss: 1.30934852\n",
      "Min Loss: 1.30934852\n",
      "Iter[1] | w[h]: Patience added: 1\n",
      "Trained weight tensor([1.], device='cuda:0', grad_fn=<SoftmaxBackward>)\n",
      "------------------------------------------------\n",
      "**** ITERATION [1] ****\n",
      "Learning ** h **\n",
      "EPOCH [1] | GLOBAL STEP [1900]\n",
      "Running Loss: 1.30904232\n",
      "Loss: 1.30904232\n",
      "Min Loss: 1.30904232\n",
      "Iter[1] | w[h]: Patience added: 1\n",
      "Trained weight tensor([1.], device='cuda:0', grad_fn=<SoftmaxBackward>)\n",
      "------------------------------------------------\n",
      "**** ITERATION [1] ****\n",
      "Learning ** h **\n",
      "EPOCH [1] | GLOBAL STEP [2000]\n",
      "Running Loss: 1.30895799\n",
      "Loss: 1.30895799\n",
      "Min Loss: 1.30895799\n",
      "Iter[1] | w[h]: Patience added: 1\n",
      "Trained weight tensor([1.], device='cuda:0', grad_fn=<SoftmaxBackward>)\n",
      "------------------------------------------------\n",
      "**** ITERATION [1] ****\n",
      "Learning ** h **\n",
      "EPOCH [1] | GLOBAL STEP [2100]\n",
      "Running Loss: 1.30876897\n",
      "Loss: 1.30876897\n",
      "Min Loss: 1.30876897\n",
      "Iter[1] | w[h]: Patience added: 1\n",
      "Trained weight tensor([1.], device='cuda:0', grad_fn=<SoftmaxBackward>)\n",
      "------------------------------------------------\n",
      "**** ITERATION [1] ****\n",
      "Learning ** h **\n",
      "EPOCH [1] | GLOBAL STEP [2200]\n",
      "Running Loss: 1.30856394\n",
      "Loss: 1.30856394\n",
      "Min Loss: 1.30856394\n",
      "Iter[1] | w[h]: Patience added: 1\n",
      "Trained weight tensor([1.], device='cuda:0', grad_fn=<SoftmaxBackward>)\n",
      "------------------------------------------------\n",
      "**** ITERATION [1] ****\n",
      "Learning ** h **\n",
      "EPOCH [1] | GLOBAL STEP [2300]\n",
      "Running Loss: 1.30835451\n",
      "Loss: 1.30835451\n",
      "Min Loss: 1.30835451\n",
      "Iter[1] | w[h]: Patience added: 1\n",
      "Trained weight tensor([1.], device='cuda:0', grad_fn=<SoftmaxBackward>)\n",
      "------------------------------------------------\n",
      "**** ITERATION [1] ****\n",
      "Learning ** h **\n",
      "EPOCH [1] | GLOBAL STEP [2400]\n",
      "Running Loss: 1.30798656\n",
      "Loss: 1.30798656\n",
      "Min Loss: 1.30798656\n",
      "Iter[1] | w[h]: Patience added: 1\n",
      "Trained weight tensor([1.], device='cuda:0', grad_fn=<SoftmaxBackward>)\n",
      "------------------------------------------------\n",
      "**** ITERATION [1] ****\n",
      "Learning ** h **\n",
      "EPOCH [1] | GLOBAL STEP [2500]\n",
      "Running Loss: 1.30805024\n",
      "Loss: 1.30805024\n",
      "Min Loss: 1.30798656\n",
      "Iter[1] | w[h]: Patience added: 2\n",
      "Trained weight tensor([1.], device='cuda:0', grad_fn=<SoftmaxBackward>)\n",
      "------------------------------------------------\n",
      "**** ITERATION [1] ****\n",
      "Learning ** h **\n",
      "EPOCH [1] | GLOBAL STEP [2600]\n",
      "Running Loss: 1.30764297\n",
      "Loss: 1.30764297\n",
      "Min Loss: 1.30764297\n",
      "Iter[1] | w[h]: Patience added: 2\n",
      "Trained weight tensor([1.], device='cuda:0', grad_fn=<SoftmaxBackward>)\n",
      "------------------------------------------------\n",
      "**** ITERATION [1] ****\n",
      "Learning ** h **\n",
      "EPOCH [1] | GLOBAL STEP [2700]\n",
      "Running Loss: 1.30791055\n",
      "Loss: 1.30791055\n",
      "Min Loss: 1.30764297\n",
      "Iter[1] | w[h]: Patience added: 3\n",
      "Trained weight tensor([1.], device='cuda:0', grad_fn=<SoftmaxBackward>)\n",
      "------------------------------------------------\n",
      "**** ITERATION [1] ****\n",
      "Learning ** h **\n",
      "EPOCH [1] | GLOBAL STEP [2800]\n",
      "Running Loss: 1.30767513\n",
      "Loss: 1.30767513\n",
      "Min Loss: 1.30764297\n",
      "Iter[1] | w[h]: Patience added: 4\n",
      "Trained weight tensor([1.], device='cuda:0', grad_fn=<SoftmaxBackward>)\n",
      "------------------------------------------------\n",
      "**** ITERATION [1] ****\n",
      "Learning ** h **\n",
      "EPOCH [1] | GLOBAL STEP [2900]\n",
      "Running Loss: 1.30754006\n",
      "Loss: 1.30754006\n",
      "Min Loss: 1.30754006\n",
      "Iter[1] | w[h]: Patience added: 4\n",
      "Trained weight tensor([1.], device='cuda:0', grad_fn=<SoftmaxBackward>)\n",
      "------------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-aaab26b0c552>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     75\u001b[0m           \u001b[0mtrained_logits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprev_logit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweaklearner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m           \u001b[0mtraining_logits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpenalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrademacher_complexity\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m           labels=labels, mode=\"train\", device=device)\n\u001b[0m\u001b[1;32m     78\u001b[0m         \u001b[0mobjective\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-341ef66ddafe>\u001b[0m in \u001b[0;36mcriterion\u001b[0;34m(trained_logits, labels, mode, penalize, lambda_, beta, weight, training_logits, device)\u001b[0m\n\u001b[1;32m      6\u001b[0m               device=torch.device('cpu')):\n\u001b[1;32m      7\u001b[0m   \u001b[0mtrained_logtis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrained_logits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m   \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m   \u001b[0mpenalize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpenalize\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   penalize_term =  torch.mul(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "base_module = BaseClassifier(upper_lambda=UPPER_LAMBDA, p=P)\n",
    "out_module = OutputModule()\n",
    "\n",
    "for t in range(1, MAX_ITER + 1):\n",
    "  if t > 1: \n",
    "    ckpt_path = base_path + \"/{}_checkpoint.pt\".format(t - 1)\n",
    "    checkpoint = torch.load(ckpt_path)\n",
    "    print(\"Load {}\".format(ckpt_path))\n",
    "    \n",
    "  h = AdaNet(t, base_module, out_module)\n",
    "  h_prime = AdaNet(t + 1, base_module, out_module)\n",
    "  \n",
    "  h = h.to(device)\n",
    "  h_prime = h_prime.to(device)\n",
    "  weaklearners = [h, h_prime]\n",
    "  \n",
    "  min_objective = {\"h\": 0., \"h_prime\": 0.}\n",
    "  for w in range(2):\n",
    "    weaklearner = weaklearners[w]\n",
    "    optimizer = optim.Adam(params=weaklearner.parameters())\n",
    "    current_w = \"h\" if w == 0 else \"h_prime\"\n",
    "    print(\"#------------------------------------------------------------#\")\n",
    "    print(\"Start {}\".format(current_w))\n",
    "    early_metrics = []\n",
    "    min_value = 0.\n",
    "    patience = 0\n",
    "    global_steps = 0\n",
    "    for epoch in range(MAX_EPOCH):\n",
    "      running_objective = 0.\n",
    "      steps_per_epoch = 1.\n",
    "      verbose_loss = 0.\n",
    "      for i, data in enumerate(trainloader):\n",
    "        global_steps += 1\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        _label = torch.zeros(\n",
    "          [TRAIN_BATCH_SIZE, NUM_CLASSES], dtype=torch.float32)\n",
    "        _label[range(_label.shape[0]), labels] = 1\n",
    "        labels = _label\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if t == 1:\n",
    "          prev_logit = torch.tensor(0.).to(device)\n",
    "          prev_weight = torch.tensor(0.).to(device)\n",
    "        else:\n",
    "          prev_modelParams = [param for param in checkpoint['model_state_dict']]\n",
    "          prev_modelWeight = {\n",
    "            k: nn.Softmax(dim=0)(v) for k, v in checkpoint['model_state_dict'].items()\n",
    "            if k == 'weight'}\n",
    "          param_locate = [param for param in weaklearner.state_dict()]\n",
    "          prev_paramIdx = [param_locate.index(i) for i in prev_modelParams]\n",
    "          prev_paramIdx.remove(0)\n",
    "\n",
    "          idx = 0\n",
    "          for param in weaklearner.parameters():\n",
    "            if idx in prev_paramIdx:\n",
    "              param.requires_grad = False\n",
    "            else:\n",
    "              param.requires_grad = True\n",
    "            idx += 1\n",
    "          \n",
    "          if checkpoint['h_or_hprime'] == 'h_prime':\n",
    "            prev_f = AdaNet(t, base_module, out_module)\n",
    "          else:\n",
    "            prev_f = AdaNet(t - 1, base_module, out_module)\n",
    "          prev_f = prev_f.to(device)\n",
    "          # In minimize F(w, u) step, Have to use previous(trained) parameters.\n",
    "          prev_f.load_state_dict(checkpoint['model_state_dict'], strict=False)\n",
    "          prev_logit, _ = prev_f(inputs)\n",
    "          prev_weight = checkpoint['model_state_dict']['weight'].to(torch.device('cpu'))\n",
    "          prev_weight = nn.Softmax(dim=0)(prev_weight)\n",
    "          \n",
    "        logits, rademacher_complexity = weaklearner(inputs)\n",
    "        objective = criterion(\n",
    "          trained_logits=prev_logit, weight=weaklearner.weight,\n",
    "          training_logits=logits, penalize=rademacher_complexity,  \n",
    "          labels=labels, mode=\"train\", device=device)\n",
    "        objective.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "        \n",
    "        steps_per_epoch += 1\n",
    "        running_objective += objective.item()\n",
    "        del inputs, labels, data, logits, _label\n",
    "        \n",
    "        verbose_loss += objective.item()\n",
    "        if i % 100 == 99:\n",
    "          if patience >= MAX_PATIENCE:\n",
    "            break\n",
    "          #_metric = objective.item()\n",
    "          _metric = verbose_loss / 100\n",
    "          if len(early_metrics) < 2:\n",
    "            early_metrics.append(_metric)\n",
    "          elif len(early_metrics) >= 2:\n",
    "            if _metric + THRESHOLD >= min_value:\n",
    "              patience += 1\n",
    "            early_metrics.append(_metric)\n",
    "            min_value = min(early_metrics)\n",
    "            early_metrics.sort()\n",
    "            early_metrics = early_metrics[:2]\n",
    "        \n",
    "          print(\"**** ITERATION [{}] ****\".format(t))\n",
    "          if w == 0: \n",
    "            print(\"Learning ** h **\")\n",
    "          else:\n",
    "            print(\"Learning ** h_prime **\")\n",
    "          print(\n",
    "            \"EPOCH [{}] | GLOBAL STEP [{}]\".format(epoch + 1, global_steps))\n",
    "          print(\"Running Loss: {0:.8f}\".format(verbose_loss / 100))\n",
    "          print(\"Loss: {0:.8f}\".format(verbose_loss / 100))\n",
    "          print(\"Min Loss: {0:.8f}\".format(min_value))\n",
    "          print(\"Iter[{}] | w[{}]: Patience added: {}\"\\\n",
    "                .format(t, current_w, patience))\n",
    "          print(\"Trained weight\", nn.Softmax(dim=0)(weaklearner.weight))\n",
    "          print(\"------------------------------------------------\")\n",
    "          verbose_loss = 0.\n",
    "        \n",
    "          \n",
    "    min_objective[current_w] = min_value\n",
    "    print(\"#################################################################\")\n",
    "    print(\"Training end in global step {}\".format(global_steps))\n",
    "    print(\"Minimum objective: {0:.8f}\".format(min_objective[current_w]))\n",
    "    print(\"[{}] end.\".format(current_w))\n",
    "    print(\"#################################################################\")\n",
    "\n",
    "  print(\"Eval h and h_prime\")\n",
    "  if min_objective[\"h_prime\"] >= min_objective[\"h\"]:\n",
    "    h_t = h\n",
    "  else:\n",
    "    h_t = h_prime\n",
    "  \n",
    "  h_t = h_t.to(torch.device('cpu'))\n",
    "  weight_star = nn.Softmax(dim=0)(h_t.weight)\n",
    "  print(\"weight_star\", weight_star)\n",
    "  if prev_weight == 0:\n",
    "    weight_total = torch.add(weight_star, nn.Softmax(dim=0)(prev_weight))\n",
    "  else:\n",
    "    if weight_star.shape == prev_weight.shape:\n",
    "      weight_total = torch.add(weight_star, nn.Softmax(dim=0)(prev_weight))\n",
    "    else:\n",
    "      zero_pad_size = weight_star.shape[0] - prev_weight.shape[0]\n",
    "      weight_trained = F.pad(\n",
    "        prev_weight, (0, zero_pad_size), 'constant', 0)\n",
    "      print(\"Prvious weight\", prev_weight)\n",
    "      weight_total = torch.add(weight_star, nn.Softmax(dim=0)(prev_weight))\n",
    "  print(\"weight total\", weight_total, \"weight prev\", prev_weight)\n",
    "  print(\"End combined weight gen.\")\n",
    "\n",
    "  val_i = 1\n",
    "  val_total = 0.\n",
    "  val_prev = 0.\n",
    "  for val_i, val_data in enumerate(valloader):\n",
    "    val_inputs, val_labels = val_data\n",
    "    val_inputs = val_inputs.to(device)\n",
    "    val_labels = val_labels.to(device)\n",
    "    _label = torch.zeros([VAL_BATCH_SIZE, NUM_CLASSES], dtype=torch.float32)\n",
    "    _label[range(_label.shape[0]), val_labels] = 1\n",
    "    val_labels = _label\n",
    "\n",
    "    case_a = copy.deepcopy(h_t) # weight_total\n",
    "    case_b = copy.deepcopy(h_t) # previouse weight\n",
    "    case_a = case_a.to(device)\n",
    "    case_b = case_b.to(device)\n",
    "\n",
    "    case_a.load_state_dict({'weight': nn.Softmax(dim=0)(weight_total)}, strict=False)\n",
    "    if t == 1:\n",
    "      case_b.load_state_dict({'weight': nn.Softmax(dim=0)(torch.Tensor([0., 0.]))}, strict=False)\n",
    "    else:\n",
    "      case_b.load_state_dict({'weight': nn.Softmax(dim=0)(prev_weight)}, strict=False)\n",
    "    \n",
    "    logit_total, rad_total = case_a(val_inputs)\n",
    "    logit_prev, rad_prev = case_a(val_inputs)\n",
    "    _objective_total = criterion(\n",
    "      trained_logits=logit_total,\n",
    "      labels=val_labels, weight=case_a.weight,\n",
    "      mode='eval', penalize=rad_total, device=device)\n",
    "    _objective_prev = criterion(\n",
    "      trained_logits=logit_prev, \n",
    "      labels=val_labels, weight=case_b.weight,\n",
    "      mode='eval', penalize=rad_prev, device=device)\n",
    "    val_i += 1\n",
    "    val_total += _objective_total.item()\n",
    "    val_prev += _objective_prev.item()\n",
    "    \n",
    "    del val_inputs, val_labels\n",
    "    \n",
    "  objective_prev = val_prev / val_i\n",
    "  objective_total = val_total / val_i\n",
    "  \n",
    "  print(\"Objective_prev:\", objective_prev, \"Objective_total:\", objective_total)\n",
    "  \n",
    "  if objective_prev >= objective_total:\n",
    "    f_t = copy.copy(case_a)\n",
    "    torch.save({\n",
    "      'iter': t,\n",
    "      'h_or_hprime': current_w,\n",
    "      'model_state_dict': f_t.state_dict(),\n",
    "      'min_objective': min_objective[current_w]},\n",
    "      f=base_path + \"/{}_checkpoint.pt\".format(t))\n",
    "  else:\n",
    "    f_t = copy.copy(case_b)\n",
    "    torch.save({\n",
    "      'iter': t,\n",
    "      'h_or_hprime': current_w,\n",
    "      'model_state_dict': f_t.state_dict(),\n",
    "      'min_objective': min_objective[current_w]},\n",
    "      f=base_path + \"/{}_checkpoint.pt\".format(t))\n",
    "    print(\"End iteration.\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
